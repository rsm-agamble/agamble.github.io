---
title: "Poisson Regression Examples"
author: "Alexa Gamble"
date: 05/03/2025
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---


## Blueprinty Case Study HW 2

### Introduction

Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. 

However, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.


### Data

```{python}
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

blueprinty = pd.read_csv("/home/jovyan/Desktop/Marketing Analytics Website : HW's/blueprinty.csv")
airbnb = pd.read_csv("/home/jovyan/Desktop/Marketing Analytics Website : HW's/airbnb.csv")

blueprinty
```

### Compare histograms and means of number of patents by customer status

Blueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.

```{python}
customers = blueprinty[blueprinty['iscustomer'] == 1]
non_customers = blueprinty[blueprinty['iscustomer'] == 0]

plt.figure(figsize=(10, 5))
plt.hist(customers['patents'], bins=20, alpha=0.6, label='Customers', edgecolor='black')
plt.hist(non_customers['patents'], bins=20, alpha=0.6, label='Non-Customers', edgecolor='black')
plt.xlabel('Number of Patents')
plt.ylabel('Frequency')
plt.title('Histogram of Number of Patents by Customer Status')
plt.legend()
plt.grid(True)
plt.show()

mean_customers = customers['patents'].mean()
mean_non_customers = non_customers['patents'].mean()

print(f'Customer Mean Number of Patents: {mean_customers}')
print(f'Non-Customer Mean Number of Patents: {mean_non_customers}')
```

The histogram above compares the distribution of patent counts between firms that use Blueprinty software (`iscustomer = 1`) and those that do not (`iscustomer = 0`). A few key patterns emerge:

- The distribution is **right-skewed** for both groups, with the bulk of firms holding between 0 and 6 patents.
- **Non-customers** are more concentrated around lower patent counts, particularly at 2-4 patents.
- **Customers** of Blueprinty show a relatively flatter distribution, with more firms holding moderate to higher numbers of patents.

The summary statistics support this visual trend:

- Non-customers have **3.47 patents** on average (n = 1,019)
- Customers have **4.13 patents** on average (n = 481)

Although this is a modest difference (approximately 0.66 more patents on average), it is consistent with the idea that Blueprinty customers may be more successful or more prolific in obtaining patents. However, it is also possible that other factor, such as firm age or region, may explain the difference, so further analysis is needed before drawing causal conclusions.


### Compare regions and ages by customer status

```{python}
import seaborn as sns
import matplotlib.pyplot as plt

sns.set_style("whitegrid")

region_props = pd.crosstab(blueprinty['region'], blueprinty['iscustomer'], normalize='index').reset_index()
region_props.columns = ['Region', 'Non-Customers', 'Customers']
region_props = pd.melt(region_props, id_vars='Region', var_name='Customer Type', value_name='Proportion')

plt.figure(figsize=(8, 5))
sns.barplot(data=region_props, x='Region', y='Proportion', hue='Customer Type')
plt.title('Proportion of Firms by Region and Customer Status')
plt.ylabel('Proportion')
plt.xlabel('Region')
plt.legend(title='Customer Type')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

```{python}
plt.figure(figsize=(6, 5))
sns.boxplot(data=blueprinty, x='iscustomer', y='age')
plt.title('Age of Firms by Customer Status')
plt.xlabel('Customer Status (0 = Non-Customer, 1 = Customer)')
plt.ylabel('Years Since Incorporation')
plt.tight_layout()
plt.show()
```

The region plot above shows notable variation in customer distribution across regions:

- In most regions (Midwest, Northwest, South, Southwest), the majority of firms are **non-customers**, with Blueprinty customers making up only around 15–20% of firms.
- The **Northeast** stands out as an exception, where nearly **half of the firms** are Blueprinty customers. This suggests that Blueprinty may have a stronger market presence or better adoption in that region.

The boxplot comparing firm age by customer status shows:

- The **median age** of customers and non-customers is very similar, both around 25–27 years.
- The **spread of firm ages** is also comparable, though customers appear to have a slightly higher upper range, with some firms approaching 50 years of age.
- There’s no strong visual evidence of systematic age differences between customers and non-customers, but customers might skew slightly older on average.

Together, these plots suggest that while **age** may not be a major confounding factor, **region** could be, especially due to the Northeast's unusual customer concentration. This is important to account for in any causal claims about Blueprinty’s effectiveness.


### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.

#### Likelihood Function

For a Poisson model where $Y_i \sim \text{Poisson}(\lambda)$, the probability mass function is:

$$
f(Y_i | \lambda) = \frac{e^{-\lambda} \lambda^{Y_i}}{Y_i!}
$$

Assuming independence across $n$ observations, the **likelihood** is:

$$
L(\lambda) = \prod_{i=1}^{n} \frac{e^{-\lambda} \lambda^{Y_i}}{Y_i!}
$$

Taking the natural log of the likelihood (log-likelihood):

$$
\ell(\lambda) = \sum_{i=1}^{n} \left( -\lambda + Y_i \log(\lambda) - \log(Y_i!) \right)
$$

```{python}
from scipy.special import gammaln

def poisson_log_likelihood(lmbda, y):
    if lmbda <= 0:
        return -np.inf
    return np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1)) 

```

### Plot of Log-Likelihood Across Lambda Values

```{python}
import numpy as np

y = blueprinty['patents'].values

lambda_vals = np.linspace(0.1, 10, 200)

log_liks = [poisson_log_likelihood(lmbda, y) for lmbda in lambda_vals]

plt.figure(figsize=(8, 5))
plt.plot(lambda_vals, log_liks)
plt.title("Log-Likelihood of Poisson Model vs Lambda")
plt.xlabel("Lambda")
plt.ylabel("Log-Likelihood")
plt.grid(True)
plt.show()
```

### Solving for MLE of Lambda (Analytical Derivation)

To find the MLE of $\lambda$, we take the log-likelihood:

$$
\ell(\lambda) = \sum_{i=1}^n \left( -\lambda + Y_i \log(\lambda) - \log(Y_i!) \right)
$$

Now take the derivative with respect to $\lambda$:

$$
\frac{d\ell}{d\lambda} = \sum_{i=1}^n \left( -1 + \frac{Y_i}{\lambda} \right)
= -n + \frac{1}{\lambda} \sum_{i=1}^n Y_i
$$

Set the derivative equal to zero and solve:

$$
-n + \frac{1}{\lambda} \sum_{i=1}^n Y_i = 0
$$

$$
\frac{1}{\lambda} \sum_{i=1}^n Y_i = n
$$

$$
\lambda_{\text{MLE}} = \frac{1}{n} \sum_{i=1}^n Y_i = \bar{Y}
$$

Thus, the maximum likelihood estimate of $\lambda$ is simply the **sample mean** of $Y$, which makes intuitive sense: the Poisson distribution models counts with mean $\lambda$, so the best estimate of $\lambda$ is just the average count in the data.

### Find MLE of Lambda via Optimization

```{python}
from scipy.optimize import minimize_scalar
from scipy.special import gammaln

def neg_poisson_log_likelihood(lmbda, y):
    if lmbda <= 0:
        return np.inf
    return -np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))

y = blueprinty['patents'].values

result = minimize_scalar(neg_poisson_log_likelihood, bounds=(0.1, 10), args=(y,), method='bounded')

lambda_mle = result.x
lambda_mle
```

### Estimation of Poisson Regression Model

Next, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.

_todo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that_ $\lambda_i = e^{X_i'\beta}$. _For example:_

```
poisson_regression_likelihood <- function(beta, Y, X){
   ...
}
```
```{python}
import numpy as np
from scipy.special import gammaln
import math

def poisson_regression_likelihood(beta, X, y, alpha=0.01):
    X = np.asarray(X, dtype=float)
    beta = np.asarray(beta, dtype=float)
    y = np.asarray(y, dtype=float)

    eta = X @ beta
    eta = np.clip(eta, -100, 100)  # Prevent overflow
    lam = np.exp(eta)

    log_likelihood = np.sum(y * eta - lam - gammaln(y + 1))
    penalty = alpha * np.sum(beta**2)
    return log_likelihood - penalty


```

```{python}
import numpy as np
import pandas as pd
from scipy.optimize import minimize

blueprinty['age_squared'] = blueprinty['age'] ** 2
region_dummies = pd.get_dummies(blueprinty['region'], drop_first=True)
X = pd.concat([
    pd.Series(1, index=blueprinty.index, name='intercept'),
    blueprinty[['age', 'age_squared', 'iscustomer']],
    region_dummies
], axis=1)
Y = blueprinty['patents'].values
X_matrix = X.values.astype(float)

def neg_log_likelihood(beta, X, Y):
    return -poisson_regression_likelihood(beta, X, Y, alpha=0.01)

# Optimize to find MLE
initial_beta = np.zeros(X_matrix.shape[1])
result = minimize(neg_log_likelihood, x0=initial_beta, args=(X_matrix, Y), method='BFGS')


# Extract MLE and Hessian inverse
beta_mle = result.x
hess_inv = result.hess_inv
# Ensure Hessian inverse is array
if not isinstance(hess_inv, np.ndarray):
    hess_inv = hess_inv.todense()
hess_inv = np.asarray(hess_inv)

# Compute standard errors
std_errors = np.sqrt(np.diag(hess_inv))

# Build results table
results_df = pd.DataFrame({
    "Coefficient": beta_mle,
    "Std. Error": std_errors
}, index=X.columns)
results_df
```

```{python}
print(X_matrix.shape)
print(X_matrix.dtype)
print(np.any(np.isnan(X_matrix)))  # Should be False

```
_todo: Use your function along with R's optim() or Python's sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1's to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors._

_todo: Check your results using R's glm() function or Python sm.GLM() function._

_todo: Interpret the results._ 

_todo: What do you conclude about the effect of Blueprinty's software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences._

_todo: Check your results using R's glm() function or Python sm.GLM() function._

_todo: Interpret the results._ 

_todo: What do you conclude about the effect of Blueprinty's software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences._

<!-- ```{python}
import numpy as np
import pandas as pd
from patsy import dmatrix
from scipy.optimize import minimize
from scipy.special import gammaln
import math

# Assuming `blueprinty` is a pandas DataFrame already loaded
blueprinty['age2'] = blueprinty['age'] ** 2

# Create region dummies, dropping the first category to avoid multicollinearity
region_dummies = pd.get_dummies(blueprinty['region'], drop_first=True)

# Construct X matrix
X = pd.concat([
    pd.Series(1, index=blueprinty.index, name='Intercept'),
    blueprinty[['age', 'age2']],
    region_dummies,
    blueprinty[['iscustomer']]
], axis=1)

# Response variable
Y = blueprinty['patents'].values

X_mat = X.values.astype(float)  # enforce numeric dtype

init_beta = np.zeros(X_mat.shape[1], dtype=float)

# Step 2: Define negative log-likelihood function
def neg_loglik(beta, Y, X):
    eta = X @ beta
    lambda_ = np.exp(eta)
    return -np.sum(Y * np.log(lambda_) - lambda_ - gammaln(Y + 1))


# Step 3: Optimize
X_mat = X.values
init_beta = np.zeros(X_mat.shape[1])
res = minimize(
    neg_loglik,
    init_beta,
    args=(Y, X_mat),
    method='BFGS',
    options={'disp': False}
)

# Step 4: Coefficients and standard errors
beta_hat = res.x
hessian_inv = res.hess_inv
se_beta = np.sqrt(np.diag(hessian_inv))

# Step 5: Coefficient table
coef_table = pd.DataFrame({
    'Term': X.columns,
    'Coefficient': beta_hat,
    'Std_Error': se_beta
})

# Display table (formatted output)
print(coef_table.round(3).to_string(index=False))

``` -->



## AirBnB Case Study

### Introduction

AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:

:::: {.callout-note collapse="true"}
### Variable Definitions

    - `id` = unique ID number for each unit
    - `last_scraped` = date when information scraped
    - `host_since` = date when host first listed the unit on Airbnb
    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed
    - `room_type` = Entire home/apt., Private room, or Shared room
    - `bathrooms` = number of bathrooms
    - `bedrooms` = number of bedrooms
    - `price` = price per night (dollars)
    - `number_of_reviews` = number of reviews for the unit on Airbnb
    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)
    - `review_scores_location` = a "quality of location" score from reviews (1-10)
    - `review_scores_value` = a "quality of value" score from reviews (1-10)
    - `instant_bookable` = "t" if instantly bookable, "f" if not

::::


_todo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided._

#### Data Cleaning and Preparation

```{python}
columns_needed = [
    "number_of_reviews", "days", "room_type", "bathrooms", "bedrooms", "price",
    "review_scores_cleanliness", "review_scores_location", "review_scores_value",
    "instant_bookable"
]
airbnb = airbnb[columns_needed].dropna()

# Convert 'instant_bookable' to binary
airbnb["instant_bookable"] = (airbnb["instant_bookable"] == "t").astype(int)

# One-hot encode 'room_type' (drop first category)
airbnb = pd.get_dummies(airbnb, columns=["room_type"], drop_first=True)
```

#### Explatory Data Analysis 

```{python}
import matplotlib.pyplot as plt
import seaborn as sns

# Histogram of reviews
sns.histplot(airbnb["number_of_reviews"], bins=50)
plt.title("Distribution of Number of Reviews")
plt.xlabel("Number of Reviews")
plt.ylabel("Count")
plt.show()

# Reviews by room type
sns.boxplot(x="room_type_Private room", y="number_of_reviews", data=airbnb)
plt.title("Reviews by Room Type (Private Room Indicator)")
plt.show()
```

#### Poisson Regression Model

```{python}
from scipy.optimize import minimize
from scipy.special import gammaln

# Define Poisson log-likelihood function
def poisson_log_likelihood(beta, Y, X):
    lambda_i = np.exp(X @ beta)
    return -np.sum(Y * np.log(lambda_i) - lambda_i - gammaln(Y + 1))

# Define response and predictors
Y = airbnb["number_of_reviews"].to_numpy()
predictor_cols = [
    "days", "bathrooms", "bedrooms", "price", "review_scores_cleanliness",
    "review_scores_location", "review_scores_value", "instant_bookable"
] + [col for col in airbnb.columns if col.startswith("room_type_")]

X = airbnb[predictor_cols]
X.insert(0, "intercept", 1.0)
X = X.to_numpy(dtype=float)

# Estimate MLE via optimization
initial_beta = np.zeros(X.shape[1])
result = minimize(poisson_log_likelihood, initial_beta, args=(Y, X), method='BFGS')

beta_hat = result.x
standard_errors = np.sqrt(np.diag(result.hess_inv))
```

#### Coefficient Table (WRONG!!!!!)

```{python}
coef_table = pd.DataFrame({
    "Coefficient": beta_hat,
    "Std. Error": standard_errors
}, index=["Intercept"] + predictor_cols)

coef_table
```
