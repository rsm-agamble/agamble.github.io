---
title: "Poisson Regression Examples"
author: "Alexa Gamble"
date: 05/03/2025
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---


## Blueprinty Case Study HW 2

### Introduction

Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. 

However, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.


### Data

```{python}
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

blueprinty = pd.read_csv("/home/jovyan/Desktop/Marketing Analytics Website : HW's/blueprinty.csv")
airbnb = pd.read_csv("/home/jovyan/Desktop/Marketing Analytics Website : HW's/airbnb.csv")

blueprinty
```

### Compare histograms and means of number of patents by customer status

Blueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.

```{python}
customers = blueprinty[blueprinty['iscustomer'] == 1]
non_customers = blueprinty[blueprinty['iscustomer'] == 0]

plt.figure(figsize=(10, 5))
plt.hist(customers['patents'], bins=20, alpha=0.6, label='Customers', edgecolor='black')
plt.hist(non_customers['patents'], bins=20, alpha=0.6, label='Non-Customers', edgecolor='black')
plt.xlabel('Number of Patents')
plt.ylabel('Frequency')
plt.title('Histogram of Number of Patents by Customer Status')
plt.legend()
plt.grid(True)
plt.show()

mean_customers = customers['patents'].mean()
mean_non_customers = non_customers['patents'].mean()

print(f'Customer Mean Number of Patents: {mean_customers}')
print(f'Non-Customer Mean Number of Patents: {mean_non_customers}')
```

The histogram above compares the distribution of patent counts between firms that use Blueprinty software (`iscustomer = 1`) and those that do not (`iscustomer = 0`). A few key patterns emerge:

- The distribution is **right-skewed** for both groups, with the bulk of firms holding between 0 and 6 patents.
- **Non-customers** are more concentrated around lower patent counts, particularly at 2-4 patents.
- **Customers** of Blueprinty show a relatively flatter distribution, with more firms holding moderate to higher numbers of patents.

The summary statistics support this visual trend:

- Non-customers have **3.47 patents** on average (n = 1,019)
- Customers have **4.13 patents** on average (n = 481)

Although this is a modest difference (approximately 0.66 more patents on average), it is consistent with the idea that Blueprinty customers may be more successful or more prolific in obtaining patents. However, it is also possible that other factor, such as firm age or region, may explain the difference, so further analysis is needed before drawing causal conclusions.


### Compare regions and ages by customer status

```{python}
import seaborn as sns
import matplotlib.pyplot as plt

sns.set_style("whitegrid")

region_props = pd.crosstab(blueprinty['region'], blueprinty['iscustomer'], normalize='index').reset_index()
region_props.columns = ['Region', 'Non-Customers', 'Customers']
region_props = pd.melt(region_props, id_vars='Region', var_name='Customer Type', value_name='Proportion')

plt.figure(figsize=(8, 5))
sns.barplot(data=region_props, x='Region', y='Proportion', hue='Customer Type')
plt.title('Proportion of Firms by Region and Customer Status')
plt.ylabel('Proportion')
plt.xlabel('Region')
plt.legend(title='Customer Type')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

```{python}
plt.figure(figsize=(6, 5))
sns.boxplot(data=blueprinty, x='iscustomer', y='age')
plt.title('Age of Firms by Customer Status')
plt.xlabel('Customer Status (0 = Non-Customer, 1 = Customer)')
plt.ylabel('Years Since Incorporation')
plt.tight_layout()
plt.show()
```

The region plot above shows notable variation in customer distribution across regions:

- In most regions (Midwest, Northwest, South, Southwest), the majority of firms are **non-customers**, with Blueprinty customers making up only around 15–20% of firms.
- The **Northeast** stands out as an exception, where nearly **half of the firms** are Blueprinty customers. This suggests that Blueprinty may have a stronger market presence or better adoption in that region.

The boxplot comparing firm age by customer status shows:

- The **median age** of customers and non-customers is very similar, both around 25–27 years.
- The **spread of firm ages** is also comparable, though customers appear to have a slightly higher upper range, with some firms approaching 50 years of age.
- There’s no strong visual evidence of systematic age differences between customers and non-customers, but customers might skew slightly older on average.

Together, these plots suggest that while **age** may not be a major confounding factor, **region** could be, especially due to the Northeast's unusual customer concentration. This is important to account for in any causal claims about Blueprinty’s effectiveness.


### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, I can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. I start by estimating a simple Poisson model via Maximum Likelihood.

#### Likelihood Function

For a Poisson model where $Y_i \sim \text{Poisson}(\lambda)$, the probability mass function is:

$$
f(Y_i | \lambda) = \frac{e^{-\lambda} \lambda^{Y_i}}{Y_i!}
$$

Assuming independence across $n$ observations, the **likelihood** is:

$$
L(\lambda) = \prod_{i=1}^{n} \frac{e^{-\lambda} \lambda^{Y_i}}{Y_i!}
$$

Taking the natural log of the likelihood (log-likelihood):

$$
\ell(\lambda) = \sum_{i=1}^{n} \left( -\lambda + Y_i \log(\lambda) - \log(Y_i!) \right)
$$

```{python}
from scipy.special import gammaln

def poisson_log_likelihood(lmbda, y):
    if lmbda <= 0:
        return -np.inf
    return np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1)) 

```

### Plot of Log-Likelihood Across Lambda Values

```{python}
y = blueprinty['patents'].values

lambda_vals = np.linspace(0.1, 10, 200)

log_liks = [poisson_log_likelihood(lmbda, y) for lmbda in lambda_vals]

plt.figure(figsize=(8, 5))
plt.plot(lambda_vals, log_liks)
plt.title("Log-Likelihood of Poisson Model vs Lambda")
plt.xlabel("Lambda")
plt.ylabel("Log-Likelihood")
plt.grid(True)
plt.show()
```

### Solving for MLE of Lambda (Analytical Derivation)

To find the MLE of $\lambda$, take the log-likelihood:

$$
\ell(\lambda) = \sum_{i=1}^n \left( -\lambda + Y_i \log(\lambda) - \log(Y_i!) \right)
$$

Now take the derivative with respect to $\lambda$:

$$
\frac{d\ell}{d\lambda} = \sum_{i=1}^n \left( -1 + \frac{Y_i}{\lambda} \right)
= -n + \frac{1}{\lambda} \sum_{i=1}^n Y_i
$$

Set the derivative equal to zero and solve:

$$
-n + \frac{1}{\lambda} \sum_{i=1}^n Y_i = 0
$$

$$
\frac{1}{\lambda} \sum_{i=1}^n Y_i = n
$$

$$
\lambda_{\text{MLE}} = \frac{1}{n} \sum_{i=1}^n Y_i = \bar{Y}
$$

Thus, the maximum likelihood estimate of $\lambda$ is simply the **sample mean** of $Y$, which makes intuitive sense: the Poisson distribution models counts with mean $\lambda$, so the best estimate of $\lambda$ is just the average count in the data.

### Find MLE of Lambda via Optimization

```{python}
from scipy.optimize import minimize_scalar

def neg_poisson_log_likelihood(lmbda, y):
    if lmbda <= 0:
        return np.inf
    return -np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))

y = blueprinty['patents'].values

result = minimize_scalar(neg_poisson_log_likelihood, bounds=(0.1, 10), args=(y,), method='bounded')

lambda_mle = result.x
lambda_mle
```

### Estimation of Poisson Regression Model

Next, extended our simple Poisson model to a Poisson Regression Model such that $Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, I will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.

```{python}
import math

def poisson_regression_likelihood(beta, X, y, alpha=0.01):
    X = np.asarray(X, dtype=float)
    beta = np.asarray(beta, dtype=float)
    y = np.asarray(y, dtype=float)

    eta = X @ beta
    eta = np.clip(eta, -100, 100)
    lam = np.exp(eta)

    log_likelihood = np.sum(y * eta - lam - gammaln(y + 1))
    penalty = alpha * np.sum(beta**2)
    return log_likelihood - penalty
```

```{python}
from scipy.optimize import minimize

blueprinty['age_squared'] = blueprinty['age'] ** 2
region_dummies = pd.get_dummies(blueprinty['region'], drop_first=True)
X = pd.concat([
    pd.Series(1, index=blueprinty.index, name='intercept'),
    blueprinty[['age', 'age_squared', 'iscustomer']],
    region_dummies
], axis=1)
Y = blueprinty['patents'].values
X_matrix = X.values.astype(float)

def neg_log_likelihood(beta, X, Y):
    return -poisson_regression_likelihood(beta, X, Y, alpha=0.01)

# Optimize to find MLE
initial_beta = np.zeros(X_matrix.shape[1])
result = minimize(neg_log_likelihood, x0=initial_beta, args=(X_matrix, Y), method='BFGS')

# Extract MLE and Hessian inverse
beta_mle = result.x
hess_inv = result.hess_inv

if not isinstance(hess_inv, np.ndarray):
    hess_inv = hess_inv.todense()
hess_inv = np.asarray(hess_inv)

std_errors = np.sqrt(np.diag(hess_inv))

results_df = pd.DataFrame({
    "Coefficient": beta_mle,
    "Std. Error": std_errors
}, index=X.columns)
results_df
```

```{python}
import statsmodels.api as sm

X_sm = sm.add_constant(X_matrix, has_constant='add')
y_sm = Y

# Fit Poisson GLM
model = sm.GLM(y_sm, X_sm, family=sm.families.Poisson())
results = model.fit()

print(results.summary())
```


#### Results and Interpretation

Estimated a Poisson regression model to examine the factors influencing the number of patents. The model includes predictors for age (both linear and squared), customer status, and region.

**Key findings:**

- **Age**  
  The coefficient for age is `0.1487`, which is statistically significant (*p* < 0.01). Interpreted on the log scale, this suggests that each additional year of age increases the expected number of patents by approximately **16%** (`exp(0.1487) ≈ 1.16`), holding other variables constant.

- **Age squared**  
  The coefficient for age squared is `-0.0030` and statistically significant. This negative value indicates a **diminishing marginal return to age**—the effect of age on patent count decreases as individuals grow older.

- **Customer status**  
  The coefficient for `iscustomer` is `0.2076`, meaning customers are expected to have approximately **23% more patents** than non-customers (`exp(0.2076) ≈ 1.23`). This effect is also statistically significant.

- **Region**  
  The regional dummy variables (Northeast, Northwest, South, Southwest) are **not statistically significant** (*p* > 0.05), suggesting that once age and customer status are accounted for, geographic region does not have a strong impact on patenting activity.

- **Model fit**  
  - Log-Likelihood: `-3258.1`  
  - Pseudo R-squared (Cragg-Uhler): `0.136`  
  - Deviance: `2143.3`  
  - Pearson chi-squared: `2070`

  These metrics suggest a **modest fit**. The relatively high Pearson chi-squared statistic hints at potential overdispersion, which may justify trying a Negative Binomial model in future work.

**Conclusion:**  
Age and customer status are strong and statistically significant predictors of patent activity. The relationship with age is nonlinear, and customers tend to file more patents than non-customers. In contrast, regional effects are weak and not statistically meaningful in this model.



#### Estimated Effect of Blueprinty's Software on Patent Counts

```{python}

X_0 = X_matrix.copy()
X_1 = X_matrix.copy()

iscustomer_index = X.columns.get_loc('iscustomer') 

X_0[:, iscustomer_index] = 0
X_1[:, iscustomer_index] = 1

beta_hat = result.x

eta_0 = X_0 @ beta_hat
eta_1 = X_1 @ beta_hat

eta_0 = np.clip(eta_0, -100, 100) 
eta_1 = np.clip(eta_1, -100, 100)

y_pred_0 = np.exp(eta_0)
y_pred_1 = np.exp(eta_1)

delta = y_pred_1 - y_pred_0
average_effect = np.mean(delta)

print(f"Average effect of Blueprinty’s software: {average_effect:.4f} more patents per firm")
```


To estimate the causal impact of using Blueprinty's software on patenting activity, simulated two counterfactual scenarios:

- `X_0`: All firms are set as non-customers (`iscustomer = 0`)
- `X_1`: All firms are set as customers (`iscustomer = 1`)

Using our fitted Poisson regression model, predicted the expected number of patents under each scenario and computed the average difference across all firms.

**Result**: On average, firms using Blueprinty's software are predicted to file **0.7928 more patents** than those who do not, holding all other variables constant.

This suggests a positive and meaningful effect of the software on innovation outcomes.



## AirBnB Case Study

### Introduction

AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:

:::: {.callout-note collapse="true"}
### Variable Definitions

    - `id` = unique ID number for each unit
    - `last_scraped` = date when information scraped
    - `host_since` = date when host first listed the unit on Airbnb
    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed
    - `room_type` = Entire home/apt., Private room, or Shared room
    - `bathrooms` = number of bathrooms
    - `bedrooms` = number of bedrooms
    - `price` = price per night (dollars)
    - `number_of_reviews` = number of reviews for the unit on Airbnb
    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)
    - `review_scores_location` = a "quality of location" score from reviews (1-10)
    - `review_scores_value` = a "quality of value" score from reviews (1-10)
    - `instant_bookable` = "t" if instantly bookable, "f" if not

::::


_todo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided._

### Data Cleaning and Preparation

```{python}
columns_needed = [
    "number_of_reviews", "days", "room_type", "bathrooms", "bedrooms", "price",
    "review_scores_cleanliness", "review_scores_location", "review_scores_value",
    "instant_bookable"
]
airbnb = airbnb[columns_needed].dropna()

# Convert 'instant_bookable' to binary
airbnb["instant_bookable"] = (airbnb["instant_bookable"] == "t").astype(int)

# One-hot encode 'room_type' (drop first category)
airbnb = pd.get_dummies(airbnb, columns=["room_type"], drop_first=True)
```

### Explatory Data Analysis 

```{python}
import matplotlib.pyplot as plt
import seaborn as sns

# Histogram of reviews
sns.histplot(airbnb["number_of_reviews"], bins=50)
plt.title("Distribution of Number of Reviews")
plt.xlabel("Number of Reviews")
plt.ylabel("Count")
plt.show()

# Reviews by room type
sns.boxplot(x="room_type_Private room", y="number_of_reviews", data=airbnb)
plt.title("Reviews by Room Type (Private Room Indicator)")
plt.show()
```

### Poisson Regression Model
```{python}
import statsmodels.api as sm

# Define predictors and target
X = airbnb.drop(columns=["number_of_reviews"])
X = sm.add_constant(X)
X = X.astype(float)  # Ensure all columns are numeric

y = airbnb["number_of_reviews"]

# Fit Poisson model
model = sm.GLM(y, X, family=sm.families.Poisson())
results = model.fit()

print(results.summary())
```

```{python}
# Extract model parameters
beta_mle = results.params.values
std_errors = results.bse.values

# Get predictor names from the model
predictor_names = results.model.exog_names

# Create coefficient matrix
results_df = pd.DataFrame({
    "Coefficient": beta_mle,
    "Std. Error": std_errors
}, index=predictor_names)

# Optional: round for cleaner display
results_df = results_df.round(4)

# Show it
results_df
```

### Interpreting Model Coefficients: Variation in Number of Reviews

Estimated a Poisson regression model using listing-level data from AirBnB in New York City. The dependent variable is the number of reviews, which I use as a proxy for the number of bookings. The predictors include listing characteristics (price, bedrooms, bathrooms), review scores, and binary indicators for booking features and room types.

**Key coefficient interpretations:**

- **Intercept (`const`) = 3.498**  
  This is the baseline log expected number of reviews for a listing with all numeric features at zero and the reference categories for categorical variables. While not directly interpretable, it anchors the model.

- **Days active (`days`) = 0.00005**  
  Each additional day a listing has been active increases the expected number of reviews by about **0.005%** (`exp(0.0000507) ≈ 1.00005`), holding all other variables constant.

- **Bathrooms = -0.1177**  
  Listings with more bathrooms are associated with **~11% fewer reviews** per additional bathroom (`exp(-0.1177) ≈ 0.889`). This may reflect less frequently booked luxury or multi-bathroom units.

- **Bedrooms = 0.0741**  
  Each additional bedroom corresponds to an **increase of about 7.7%** in the expected number of reviews (`exp(0.0741) ≈ 1.077`).

- **Price = -0.000018**  
  A $1 increase in nightly price is associated with a very slight **decrease in expected reviews**, though the effect is economically negligible. This suggests price sensitivity is limited or nonlinear.

- **Review scores:**
  - **Cleanliness = 0.1131**  
    Listings rated 1 point higher on cleanliness receive **~12% more reviews** (`exp(0.1131) ≈ 1.12`).
  - **Location = -0.0769**  
    Surprisingly, higher location scores are associated with **~7.4% fewer reviews**, possibly due to multicollinearity or non-monotonic guest expectations.
  - **Value = -0.0911**  
    A higher value score is associated with **~8.7% fewer reviews**, which again may reflect a complex relationship between ratings and review volume.

- **Instant bookable = 0.3459**  
  Listings that support instant booking receive **~41% more reviews** on average (`exp(0.3459) ≈ 1.41`), highlighting the importance of convenience in booking behavior.

- **Room type (reference: Entire home/apt):**
  - **Private room = -0.0105**  
    Slightly fewer reviews, about **1% less** than entire homes.
  - **Shared room = -0.2463**  
    Significantly fewer reviews, with **~22% less activity** compared to entire homes.

Overall, listing convenience (e.g., instant bookable), cleanliness, and size (bedrooms) are strongly associated with greater booking activity. Room type and days listed also matter, while the effect of price and some review scores are more subtle.

