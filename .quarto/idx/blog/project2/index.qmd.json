{"title":"Poisson Regression Examples","markdown":{"yaml":{"title":"Poisson Regression Examples","author":"Alexa Gamble","date":"05/03/2025","callout-appearance":"minimal"},"headingText":"Blueprinty Case Study HW 2","containsRefs":false,"markdown":"\n\n\n\n### Introduction\n\nBlueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. \n\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.\n\n\n### Data\n\n```{python}\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nblueprinty = pd.read_csv(\"/home/jovyan/Desktop/Marketing Analytics Website : HW's/blueprinty.csv\")\nairbnb = pd.read_csv(\"/home/jovyan/Desktop/Marketing Analytics Website : HW's/airbnb.csv\")\n\nblueprinty\n```\n\n### Compare histograms and means of number of patents by customer status\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n```{python}\ncustomers = blueprinty[blueprinty['iscustomer'] == 1]\nnon_customers = blueprinty[blueprinty['iscustomer'] == 0]\n\nplt.figure(figsize=(10, 5))\nplt.hist(customers['patents'], bins=20, alpha=0.6, label='Customers', edgecolor='black')\nplt.hist(non_customers['patents'], bins=20, alpha=0.6, label='Non-Customers', edgecolor='black')\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histogram of Number of Patents by Customer Status')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nmean_customers = customers['patents'].mean()\nmean_non_customers = non_customers['patents'].mean()\n\nprint(f'Customer Mean Number of Patents: {mean_customers}')\nprint(f'Non-Customer Mean Number of Patents: {mean_non_customers}')\n```\n\nThe histogram above compares the distribution of patent counts between firms that use Blueprinty software (`iscustomer = 1`) and those that do not (`iscustomer = 0`). A few key patterns emerge:\n\n- The distribution is **right-skewed** for both groups, with the bulk of firms holding between 0 and 6 patents.\n- **Non-customers** are more concentrated around lower patent counts, particularly at 2-4 patents.\n- **Customers** of Blueprinty show a relatively flatter distribution, with more firms holding moderate to higher numbers of patents.\n\nThe summary statistics support this visual trend:\n\n- Non-customers have **3.47 patents** on average (n = 1,019)\n- Customers have **4.13 patents** on average (n = 481)\n\nAlthough this is a modest difference (approximately 0.66 more patents on average), it is consistent with the idea that Blueprinty customers may be more successful or more prolific in obtaining patents. However, it is also possible that other factor, such as firm age or region, may explain the difference, so further analysis is needed before drawing causal conclusions.\n\n\n### Compare regions and ages by customer status\n\n```{python}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_style(\"whitegrid\")\n\nregion_props = pd.crosstab(blueprinty['region'], blueprinty['iscustomer'], normalize='index').reset_index()\nregion_props.columns = ['Region', 'Non-Customers', 'Customers']\nregion_props = pd.melt(region_props, id_vars='Region', var_name='Customer Type', value_name='Proportion')\n\nplt.figure(figsize=(8, 5))\nsns.barplot(data=region_props, x='Region', y='Proportion', hue='Customer Type')\nplt.title('Proportion of Firms by Region and Customer Status')\nplt.ylabel('Proportion')\nplt.xlabel('Region')\nplt.legend(title='Customer Type')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n```\n\n```{python}\nplt.figure(figsize=(6, 5))\nsns.boxplot(data=blueprinty, x='iscustomer', y='age')\nplt.title('Age of Firms by Customer Status')\nplt.xlabel('Customer Status (0 = Non-Customer, 1 = Customer)')\nplt.ylabel('Years Since Incorporation')\nplt.tight_layout()\nplt.show()\n```\n\nThe region plot above shows notable variation in customer distribution across regions:\n\n- In most regions (Midwest, Northwest, South, Southwest), the majority of firms are **non-customers**, with Blueprinty customers making up only around 15–20% of firms.\n- The **Northeast** stands out as an exception, where nearly **half of the firms** are Blueprinty customers. This suggests that Blueprinty may have a stronger market presence or better adoption in that region.\n\nThe boxplot comparing firm age by customer status shows:\n\n- The **median age** of customers and non-customers is very similar, both around 25–27 years.\n- The **spread of firm ages** is also comparable, though customers appear to have a slightly higher upper range, with some firms approaching 50 years of age.\n- There’s no strong visual evidence of systematic age differences between customers and non-customers, but customers might skew slightly older on average.\n\nTogether, these plots suggest that while **age** may not be a major confounding factor, **region** could be, especially due to the Northeast's unusual customer concentration. This is important to account for in any causal claims about Blueprinty’s effectiveness.\n\n\n### Estimation of Simple Poisson Model\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, I can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. I start by estimating a simple Poisson model via Maximum Likelihood.\n\n#### Likelihood Function\n\nFor a Poisson model where $Y_i \\sim \\text{Poisson}(\\lambda)$, the probability mass function is:\n\n$$\nf(Y_i | \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n$$\n\nAssuming independence across $n$ observations, the **likelihood** is:\n\n$$\nL(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n$$\n\nThis is then simplified to:\n\n$$\nL(\\lambda) = \\frac{e^{-n\\lambda} \\lambda^{\\sum Y_i}}{\\prod Y_i!}\n$$\n\n```{python}\nfrom scipy.special import gammaln\n\ndef poisson_log_likelihood(lmbda, y):\n    if lmbda <= 0:\n        return -np.inf\n    return np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1)) \n\n```\n\n### Plot of Log-Likelihood Across Lambda Values\n\n```{python}\ny = blueprinty['patents'].values\n\nlambda_vals = np.linspace(0.1, 10, 200)\n\nlog_liks = [poisson_log_likelihood(lmbda, y) for lmbda in lambda_vals]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, log_liks)\nplt.title(\"Log-Likelihood of Poisson Model vs Lambda\")\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.grid(True)\nplt.show()\n```\n\n### Solving for MLE of Lambda (Analytical Derivation)\n\nTo find the MLE of $\\lambda$, take the log-likelihood:\n\n$$\n\\ell(\\lambda) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right)\n$$\n\nNow take the derivative with respect to $\\lambda$:\n\n$$\n\\frac{d\\ell}{d\\lambda} = \\sum_{i=1}^n \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n= -n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i\n$$\n\nSet the derivative equal to zero and solve:\n\n$$\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = 0\n$$\n\n$$\n\\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = n\n$$\n\n$$\n\\lambda_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n$$\n\nThus, the maximum likelihood estimate of $\\lambda$ is simply the **sample mean** of $Y$, which makes intuitive sense: the Poisson distribution models counts with mean $\\lambda$, so the best estimate of $\\lambda$ is just the average count in the data.\n\n### Find MLE of Lambda via Optimization\n\n```{python}\nfrom scipy.optimize import minimize_scalar\n\ndef neg_poisson_log_likelihood(lmbda, y):\n    if lmbda <= 0:\n        return np.inf\n    return -np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))\n\ny = blueprinty['patents'].values\n\nresult = minimize_scalar(neg_poisson_log_likelihood, bounds=(0.1, 10), args=(y,), method='bounded')\n\nlambda_mle = result.x\nlambda_mle\n```\n\n### Estimation of Poisson Regression Model\n\nNext, extended our simple Poisson model to a Poisson Regression Model such that $Y_i = \\text{Poisson}(\\lambda_i)$ where $\\lambda_i = \\exp(X_i'\\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, I will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n```{python}\nimport math\n\ndef poisson_regression_likelihood(beta, X, y, alpha=0.01):\n    X = np.asarray(X, dtype=float)\n    beta = np.asarray(beta, dtype=float)\n    y = np.asarray(y, dtype=float)\n\n    eta = X @ beta\n    eta = np.clip(eta, -100, 100)\n    lam = np.exp(eta)\n\n    log_likelihood = np.sum(y * eta - lam - gammaln(y + 1))\n    penalty = alpha * np.sum(beta**2)\n    return log_likelihood - penalty\n```\n\n```{python}\nfrom scipy.optimize import minimize\n\nblueprinty['age_squared'] = blueprinty['age'] ** 2\nregion_dummies = pd.get_dummies(blueprinty['region'], drop_first=True)\nX = pd.concat([\n    pd.Series(1, index=blueprinty.index, name='intercept'),\n    blueprinty[['age', 'age_squared', 'iscustomer']],\n    region_dummies\n], axis=1)\nY = blueprinty['patents'].values\nX_matrix = X.values.astype(float)\n\ndef neg_log_likelihood(beta, X, Y):\n    return -poisson_regression_likelihood(beta, X, Y, alpha=0.01)\n\n# Optimize to find MLE\ninitial_beta = np.zeros(X_matrix.shape[1])\nresult = minimize(neg_log_likelihood, x0=initial_beta, args=(X_matrix, Y), method='BFGS')\n\n# Extract MLE and Hessian inverse\nbeta_mle = result.x\nhess_inv = result.hess_inv\n\nif not isinstance(hess_inv, np.ndarray):\n    hess_inv = hess_inv.todense()\nhess_inv = np.asarray(hess_inv)\n\nstd_errors = np.sqrt(np.diag(hess_inv))\n\nresults_df = pd.DataFrame({\n    \"Coefficient\": beta_mle,\n    \"Std. Error\": std_errors\n}, index=X.columns)\nresults_df\n```\n\n```{python}\nimport statsmodels.api as sm\n\n# Ensure all covariates are numeric\nX_numeric = X.astype(float)\nY_numeric = Y.astype(float)\n\npoisson_model = sm.GLM(Y_numeric, X_numeric, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\nprint(poisson_results.summary())\n\n# And to extract coeffs and SEs:\nimport pandas as pd\nresult_table = pd.DataFrame({\n    'coef': poisson_results.params,\n    'std_err': poisson_results.bse\n})\nprint(result_table)\n```\n\n\n#### Results and Interpretation\n\nEstimated a Poisson regression model to examine the factors influencing the number of patents. The model includes predictors for age (both linear and squared), customer status, and region.\n\n**Key findings:**\n\n- **Age**  \n  The coefficient for age is `0.1487`, which is statistically significant (*p* < 0.01). Interpreted on the log scale, this suggests that each additional year of age increases the expected number of patents by approximately **16%** (`exp(0.1487) ≈ 1.16`), holding other variables constant.\n\n- **Age squared**  \n  The coefficient for age squared is `-0.0030` and statistically significant. This negative value indicates a **diminishing marginal return to age**—the effect of age on patent count decreases as individuals grow older.\n\n- **Customer status**  \n  The coefficient for `iscustomer` is `0.2076`, meaning customers are expected to have approximately **23% more patents** than non-customers (`exp(0.2076) ≈ 1.23`). This effect is also statistically significant.\n\n- **Region**  \n  The regional dummy variables (Northeast, Northwest, South, Southwest) are **not statistically significant** (*p* > 0.05), suggesting that once age and customer status are accounted for, geographic region does not have a strong impact on patenting activity.\n\n- **Model fit**  \n  - Log-Likelihood: `-3258.1`  \n  - Pseudo R-squared (Cragg-Uhler): `0.136`  \n  - Deviance: `2143.3`  \n  - Pearson chi-squared: `2070`\n\n  These metrics suggest a **modest fit**. The relatively high Pearson chi-squared statistic hints at potential overdispersion, which may justify trying a Negative Binomial model in future work.\n\n**Conclusion:**  \nAge and customer status are strong and statistically significant predictors of patent activity. The relationship with age is nonlinear, and customers tend to file more patents than non-customers. In contrast, regional effects are weak and not statistically meaningful in this model.\n\n\n\n#### Estimated Effect of Blueprinty's Software on Patent Counts\n\n```{python}\n\nX_0 = X_matrix.copy()\nX_1 = X_matrix.copy()\n\niscustomer_index = X.columns.get_loc('iscustomer') \n\nX_0[:, iscustomer_index] = 0\nX_1[:, iscustomer_index] = 1\n\nbeta_hat = result.x\n\neta_0 = X_0 @ beta_hat\neta_1 = X_1 @ beta_hat\n\neta_0 = np.clip(eta_0, -100, 100) \neta_1 = np.clip(eta_1, -100, 100)\n\ny_pred_0 = np.exp(eta_0)\ny_pred_1 = np.exp(eta_1)\n\ndelta = y_pred_1 - y_pred_0\naverage_effect = np.mean(delta)\n\nprint(f\"Average effect of Blueprinty’s software: {average_effect:.4f} more patents per firm\")\n```\n\n\nTo estimate the causal impact of using Blueprinty's software on patenting activity, simulated two counterfactual scenarios:\n\n- `X_0`: All firms are set as non-customers (`iscustomer = 0`)\n- `X_1`: All firms are set as customers (`iscustomer = 1`)\n\nUsing our fitted Poisson regression model, predicted the expected number of patents under each scenario and computed the average difference across all firms.\n\n**Result**: On average, firms using Blueprinty's software are predicted to file **0.7928 more patents** than those who do not, holding all other variables constant.\n\nThis suggests a positive and meaningful effect of the software on innovation outcomes.\n\n\n\n## AirBnB Case Study\n\n### Introduction\n\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:\n\n:::: {.callout-note collapse=\"true\"}\n### Variable Definitions\n\n    - `id` = unique ID number for each unit\n    - `last_scraped` = date when information scraped\n    - `host_since` = date when host first listed the unit on Airbnb\n    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n    - `room_type` = Entire home/apt., Private room, or Shared room\n    - `bathrooms` = number of bathrooms\n    - `bedrooms` = number of bedrooms\n    - `price` = price per night (dollars)\n    - `number_of_reviews` = number of reviews for the unit on Airbnb\n    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n    - `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n    - `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n    - `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n::::\n\n### Data Cleaning and Preparation\n\n```{python}\ncolumns_needed = [\n    'days', 'room_type', 'bathrooms', 'bedrooms', 'price',\n    'review_scores_cleanliness', 'review_scores_location',\n    'review_scores_value', 'instant_bookable', 'number_of_reviews'\n]\nairbnb = airbnb[columns_needed].dropna()\n\nairbnb = pd.get_dummies(airbnb, columns=[\"room_type\", \"instant_bookable\"], drop_first=True)\n```\n\n### Explatory Data Analysis \n\n```{python}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Histogram of reviews\nsns.histplot(airbnb[\"number_of_reviews\"], bins=50)\nplt.title(\"Distribution of Number of Reviews\")\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Count\")\nplt.show()\n\n# Reviews by room type\nsns.boxplot(x=\"room_type_Private room\", y=\"number_of_reviews\", data=airbnb)\nplt.title(\"Reviews by Room Type (Private Room Indicator)\")\nplt.show()\n```\n\n### Poisson Regression Model\n```{python}\nX = airbnb[[\n    'days', 'bathrooms', 'bedrooms', 'price',\n    'review_scores_cleanliness', 'review_scores_location', 'review_scores_value',\n    'room_type_Private room', 'room_type_Shared room', 'instant_bookable_t'\n]]\nX = sm.add_constant(X)  # Add intercept term\nY = airbnb['number_of_reviews']\n\nX = X.astype(float)\nY = Y.astype(float)\n\n# Step 4: Fit Poisson regression model\nmodel = sm.GLM(Y, X, family=sm.families.Poisson())\nresults = model.fit()\n\nprint(results.summary())\n```\n\n### Interpreting Model Coefficients: Variation in Number of Reviews\n\nEstimated a Poisson regression model using listing-level data from AirBnB in New York City. The dependent variable is the number of reviews, which I use as a proxy for the number of bookings. The predictors include listing characteristics (price, bedrooms, bathrooms), review scores, and binary indicators for booking features and room types.\n\n#### Model Summary\n\n- **Dependent variable**: `number_of_reviews`\n- **Model family**: Poisson (log link)\n- **Observations**: 30,160\n- **Pseudo R² (Cragg & Uhler's)**: 0.684 — indicates strong explanatory power for count data.\n- **All predictors are statistically significant (p < 0.05)**\n\n#### Key Coefficient Interpretations\n\n#### Baseline (Intercept)\n- `const = 3.4980`\n  - When all predictors are 0, the expected number of reviews is:  \n    $$ \\exp(3.4980) \\approx 33 $$\n\n#### Continuous Predictors\n\n- **`days`**: 0.00005072  \n  - Each additional day listed increases expected reviews by ~0.5% per 100 days.\n\n- **`bathrooms`**: -0.1177  \n  - Each additional bathroom is associated with ~11.1% fewer reviews  \n    $$ \\exp(-0.1177) \\approx 0.889 $$\n\n- **`bedrooms`**: 0.0741  \n  - Each additional bedroom is associated with ~7.7% more reviews  \n    $$ \\exp(0.0741) \\approx 1.077 $$\n\n- **`price`**: -1.791e-5  \n  - Very small negative effect. Each additional dollar reduces expected reviews by a negligible amount.\n\n#### Review Scores\n\n- **`review_scores_cleanliness`**: 0.1131  \n  - Higher cleanliness scores → ~12% more reviews per point increase\n\n- **`review_scores_location`**: -0.0769  \n  - Higher location scores → ~7.4% fewer reviews (unexpected, may indicate multicollinearity)\n\n- **`review_scores_value`**: -0.0911  \n  - Higher value scores → ~8.7% fewer reviews\n\n#### Categorical Predictors (relative to reference category)\n\n- **`room_type_Private room`**: -0.0105  \n  - Private rooms receive ~1% fewer reviews than entire places\n\n- **`room_type_Shared room`**: -0.2463  \n  - Shared rooms receive ~22% fewer reviews  \n    $$ \\exp(-0.2463) \\approx 0.782 $$\n\n- **`instant_bookable_t`**: 0.3459  \n  - Listings that are instantly bookable receive ~41% more reviews  \n    $$ \\exp(0.3459) \\approx 1.413 $$\n\n#### Key Takeaways\n\n- Listings with **instant booking**, more **bedrooms**, and higher **cleanliness scores** tend to receive more reviews.\n- Shared rooms and higher bathroom count are associated with **fewer reviews**.\n- The negative coefficients for `location` and `value` are counterintuitive and may suggest multicollinearity or omitted variable bias.\n\n\n\n\n","srcMarkdownNoYaml":"\n\n\n## Blueprinty Case Study HW 2\n\n### Introduction\n\nBlueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. \n\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.\n\n\n### Data\n\n```{python}\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nblueprinty = pd.read_csv(\"/home/jovyan/Desktop/Marketing Analytics Website : HW's/blueprinty.csv\")\nairbnb = pd.read_csv(\"/home/jovyan/Desktop/Marketing Analytics Website : HW's/airbnb.csv\")\n\nblueprinty\n```\n\n### Compare histograms and means of number of patents by customer status\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n```{python}\ncustomers = blueprinty[blueprinty['iscustomer'] == 1]\nnon_customers = blueprinty[blueprinty['iscustomer'] == 0]\n\nplt.figure(figsize=(10, 5))\nplt.hist(customers['patents'], bins=20, alpha=0.6, label='Customers', edgecolor='black')\nplt.hist(non_customers['patents'], bins=20, alpha=0.6, label='Non-Customers', edgecolor='black')\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histogram of Number of Patents by Customer Status')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nmean_customers = customers['patents'].mean()\nmean_non_customers = non_customers['patents'].mean()\n\nprint(f'Customer Mean Number of Patents: {mean_customers}')\nprint(f'Non-Customer Mean Number of Patents: {mean_non_customers}')\n```\n\nThe histogram above compares the distribution of patent counts between firms that use Blueprinty software (`iscustomer = 1`) and those that do not (`iscustomer = 0`). A few key patterns emerge:\n\n- The distribution is **right-skewed** for both groups, with the bulk of firms holding between 0 and 6 patents.\n- **Non-customers** are more concentrated around lower patent counts, particularly at 2-4 patents.\n- **Customers** of Blueprinty show a relatively flatter distribution, with more firms holding moderate to higher numbers of patents.\n\nThe summary statistics support this visual trend:\n\n- Non-customers have **3.47 patents** on average (n = 1,019)\n- Customers have **4.13 patents** on average (n = 481)\n\nAlthough this is a modest difference (approximately 0.66 more patents on average), it is consistent with the idea that Blueprinty customers may be more successful or more prolific in obtaining patents. However, it is also possible that other factor, such as firm age or region, may explain the difference, so further analysis is needed before drawing causal conclusions.\n\n\n### Compare regions and ages by customer status\n\n```{python}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_style(\"whitegrid\")\n\nregion_props = pd.crosstab(blueprinty['region'], blueprinty['iscustomer'], normalize='index').reset_index()\nregion_props.columns = ['Region', 'Non-Customers', 'Customers']\nregion_props = pd.melt(region_props, id_vars='Region', var_name='Customer Type', value_name='Proportion')\n\nplt.figure(figsize=(8, 5))\nsns.barplot(data=region_props, x='Region', y='Proportion', hue='Customer Type')\nplt.title('Proportion of Firms by Region and Customer Status')\nplt.ylabel('Proportion')\nplt.xlabel('Region')\nplt.legend(title='Customer Type')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n```\n\n```{python}\nplt.figure(figsize=(6, 5))\nsns.boxplot(data=blueprinty, x='iscustomer', y='age')\nplt.title('Age of Firms by Customer Status')\nplt.xlabel('Customer Status (0 = Non-Customer, 1 = Customer)')\nplt.ylabel('Years Since Incorporation')\nplt.tight_layout()\nplt.show()\n```\n\nThe region plot above shows notable variation in customer distribution across regions:\n\n- In most regions (Midwest, Northwest, South, Southwest), the majority of firms are **non-customers**, with Blueprinty customers making up only around 15–20% of firms.\n- The **Northeast** stands out as an exception, where nearly **half of the firms** are Blueprinty customers. This suggests that Blueprinty may have a stronger market presence or better adoption in that region.\n\nThe boxplot comparing firm age by customer status shows:\n\n- The **median age** of customers and non-customers is very similar, both around 25–27 years.\n- The **spread of firm ages** is also comparable, though customers appear to have a slightly higher upper range, with some firms approaching 50 years of age.\n- There’s no strong visual evidence of systematic age differences between customers and non-customers, but customers might skew slightly older on average.\n\nTogether, these plots suggest that while **age** may not be a major confounding factor, **region** could be, especially due to the Northeast's unusual customer concentration. This is important to account for in any causal claims about Blueprinty’s effectiveness.\n\n\n### Estimation of Simple Poisson Model\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, I can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. I start by estimating a simple Poisson model via Maximum Likelihood.\n\n#### Likelihood Function\n\nFor a Poisson model where $Y_i \\sim \\text{Poisson}(\\lambda)$, the probability mass function is:\n\n$$\nf(Y_i | \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n$$\n\nAssuming independence across $n$ observations, the **likelihood** is:\n\n$$\nL(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n$$\n\nThis is then simplified to:\n\n$$\nL(\\lambda) = \\frac{e^{-n\\lambda} \\lambda^{\\sum Y_i}}{\\prod Y_i!}\n$$\n\n```{python}\nfrom scipy.special import gammaln\n\ndef poisson_log_likelihood(lmbda, y):\n    if lmbda <= 0:\n        return -np.inf\n    return np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1)) \n\n```\n\n### Plot of Log-Likelihood Across Lambda Values\n\n```{python}\ny = blueprinty['patents'].values\n\nlambda_vals = np.linspace(0.1, 10, 200)\n\nlog_liks = [poisson_log_likelihood(lmbda, y) for lmbda in lambda_vals]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, log_liks)\nplt.title(\"Log-Likelihood of Poisson Model vs Lambda\")\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.grid(True)\nplt.show()\n```\n\n### Solving for MLE of Lambda (Analytical Derivation)\n\nTo find the MLE of $\\lambda$, take the log-likelihood:\n\n$$\n\\ell(\\lambda) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right)\n$$\n\nNow take the derivative with respect to $\\lambda$:\n\n$$\n\\frac{d\\ell}{d\\lambda} = \\sum_{i=1}^n \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n= -n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i\n$$\n\nSet the derivative equal to zero and solve:\n\n$$\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = 0\n$$\n\n$$\n\\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = n\n$$\n\n$$\n\\lambda_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n$$\n\nThus, the maximum likelihood estimate of $\\lambda$ is simply the **sample mean** of $Y$, which makes intuitive sense: the Poisson distribution models counts with mean $\\lambda$, so the best estimate of $\\lambda$ is just the average count in the data.\n\n### Find MLE of Lambda via Optimization\n\n```{python}\nfrom scipy.optimize import minimize_scalar\n\ndef neg_poisson_log_likelihood(lmbda, y):\n    if lmbda <= 0:\n        return np.inf\n    return -np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))\n\ny = blueprinty['patents'].values\n\nresult = minimize_scalar(neg_poisson_log_likelihood, bounds=(0.1, 10), args=(y,), method='bounded')\n\nlambda_mle = result.x\nlambda_mle\n```\n\n### Estimation of Poisson Regression Model\n\nNext, extended our simple Poisson model to a Poisson Regression Model such that $Y_i = \\text{Poisson}(\\lambda_i)$ where $\\lambda_i = \\exp(X_i'\\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, I will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n```{python}\nimport math\n\ndef poisson_regression_likelihood(beta, X, y, alpha=0.01):\n    X = np.asarray(X, dtype=float)\n    beta = np.asarray(beta, dtype=float)\n    y = np.asarray(y, dtype=float)\n\n    eta = X @ beta\n    eta = np.clip(eta, -100, 100)\n    lam = np.exp(eta)\n\n    log_likelihood = np.sum(y * eta - lam - gammaln(y + 1))\n    penalty = alpha * np.sum(beta**2)\n    return log_likelihood - penalty\n```\n\n```{python}\nfrom scipy.optimize import minimize\n\nblueprinty['age_squared'] = blueprinty['age'] ** 2\nregion_dummies = pd.get_dummies(blueprinty['region'], drop_first=True)\nX = pd.concat([\n    pd.Series(1, index=blueprinty.index, name='intercept'),\n    blueprinty[['age', 'age_squared', 'iscustomer']],\n    region_dummies\n], axis=1)\nY = blueprinty['patents'].values\nX_matrix = X.values.astype(float)\n\ndef neg_log_likelihood(beta, X, Y):\n    return -poisson_regression_likelihood(beta, X, Y, alpha=0.01)\n\n# Optimize to find MLE\ninitial_beta = np.zeros(X_matrix.shape[1])\nresult = minimize(neg_log_likelihood, x0=initial_beta, args=(X_matrix, Y), method='BFGS')\n\n# Extract MLE and Hessian inverse\nbeta_mle = result.x\nhess_inv = result.hess_inv\n\nif not isinstance(hess_inv, np.ndarray):\n    hess_inv = hess_inv.todense()\nhess_inv = np.asarray(hess_inv)\n\nstd_errors = np.sqrt(np.diag(hess_inv))\n\nresults_df = pd.DataFrame({\n    \"Coefficient\": beta_mle,\n    \"Std. Error\": std_errors\n}, index=X.columns)\nresults_df\n```\n\n```{python}\nimport statsmodels.api as sm\n\n# Ensure all covariates are numeric\nX_numeric = X.astype(float)\nY_numeric = Y.astype(float)\n\npoisson_model = sm.GLM(Y_numeric, X_numeric, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\nprint(poisson_results.summary())\n\n# And to extract coeffs and SEs:\nimport pandas as pd\nresult_table = pd.DataFrame({\n    'coef': poisson_results.params,\n    'std_err': poisson_results.bse\n})\nprint(result_table)\n```\n\n\n#### Results and Interpretation\n\nEstimated a Poisson regression model to examine the factors influencing the number of patents. The model includes predictors for age (both linear and squared), customer status, and region.\n\n**Key findings:**\n\n- **Age**  \n  The coefficient for age is `0.1487`, which is statistically significant (*p* < 0.01). Interpreted on the log scale, this suggests that each additional year of age increases the expected number of patents by approximately **16%** (`exp(0.1487) ≈ 1.16`), holding other variables constant.\n\n- **Age squared**  \n  The coefficient for age squared is `-0.0030` and statistically significant. This negative value indicates a **diminishing marginal return to age**—the effect of age on patent count decreases as individuals grow older.\n\n- **Customer status**  \n  The coefficient for `iscustomer` is `0.2076`, meaning customers are expected to have approximately **23% more patents** than non-customers (`exp(0.2076) ≈ 1.23`). This effect is also statistically significant.\n\n- **Region**  \n  The regional dummy variables (Northeast, Northwest, South, Southwest) are **not statistically significant** (*p* > 0.05), suggesting that once age and customer status are accounted for, geographic region does not have a strong impact on patenting activity.\n\n- **Model fit**  \n  - Log-Likelihood: `-3258.1`  \n  - Pseudo R-squared (Cragg-Uhler): `0.136`  \n  - Deviance: `2143.3`  \n  - Pearson chi-squared: `2070`\n\n  These metrics suggest a **modest fit**. The relatively high Pearson chi-squared statistic hints at potential overdispersion, which may justify trying a Negative Binomial model in future work.\n\n**Conclusion:**  \nAge and customer status are strong and statistically significant predictors of patent activity. The relationship with age is nonlinear, and customers tend to file more patents than non-customers. In contrast, regional effects are weak and not statistically meaningful in this model.\n\n\n\n#### Estimated Effect of Blueprinty's Software on Patent Counts\n\n```{python}\n\nX_0 = X_matrix.copy()\nX_1 = X_matrix.copy()\n\niscustomer_index = X.columns.get_loc('iscustomer') \n\nX_0[:, iscustomer_index] = 0\nX_1[:, iscustomer_index] = 1\n\nbeta_hat = result.x\n\neta_0 = X_0 @ beta_hat\neta_1 = X_1 @ beta_hat\n\neta_0 = np.clip(eta_0, -100, 100) \neta_1 = np.clip(eta_1, -100, 100)\n\ny_pred_0 = np.exp(eta_0)\ny_pred_1 = np.exp(eta_1)\n\ndelta = y_pred_1 - y_pred_0\naverage_effect = np.mean(delta)\n\nprint(f\"Average effect of Blueprinty’s software: {average_effect:.4f} more patents per firm\")\n```\n\n\nTo estimate the causal impact of using Blueprinty's software on patenting activity, simulated two counterfactual scenarios:\n\n- `X_0`: All firms are set as non-customers (`iscustomer = 0`)\n- `X_1`: All firms are set as customers (`iscustomer = 1`)\n\nUsing our fitted Poisson regression model, predicted the expected number of patents under each scenario and computed the average difference across all firms.\n\n**Result**: On average, firms using Blueprinty's software are predicted to file **0.7928 more patents** than those who do not, holding all other variables constant.\n\nThis suggests a positive and meaningful effect of the software on innovation outcomes.\n\n\n\n## AirBnB Case Study\n\n### Introduction\n\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:\n\n:::: {.callout-note collapse=\"true\"}\n### Variable Definitions\n\n    - `id` = unique ID number for each unit\n    - `last_scraped` = date when information scraped\n    - `host_since` = date when host first listed the unit on Airbnb\n    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n    - `room_type` = Entire home/apt., Private room, or Shared room\n    - `bathrooms` = number of bathrooms\n    - `bedrooms` = number of bedrooms\n    - `price` = price per night (dollars)\n    - `number_of_reviews` = number of reviews for the unit on Airbnb\n    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n    - `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n    - `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n    - `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n::::\n\n### Data Cleaning and Preparation\n\n```{python}\ncolumns_needed = [\n    'days', 'room_type', 'bathrooms', 'bedrooms', 'price',\n    'review_scores_cleanliness', 'review_scores_location',\n    'review_scores_value', 'instant_bookable', 'number_of_reviews'\n]\nairbnb = airbnb[columns_needed].dropna()\n\nairbnb = pd.get_dummies(airbnb, columns=[\"room_type\", \"instant_bookable\"], drop_first=True)\n```\n\n### Explatory Data Analysis \n\n```{python}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Histogram of reviews\nsns.histplot(airbnb[\"number_of_reviews\"], bins=50)\nplt.title(\"Distribution of Number of Reviews\")\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Count\")\nplt.show()\n\n# Reviews by room type\nsns.boxplot(x=\"room_type_Private room\", y=\"number_of_reviews\", data=airbnb)\nplt.title(\"Reviews by Room Type (Private Room Indicator)\")\nplt.show()\n```\n\n### Poisson Regression Model\n```{python}\nX = airbnb[[\n    'days', 'bathrooms', 'bedrooms', 'price',\n    'review_scores_cleanliness', 'review_scores_location', 'review_scores_value',\n    'room_type_Private room', 'room_type_Shared room', 'instant_bookable_t'\n]]\nX = sm.add_constant(X)  # Add intercept term\nY = airbnb['number_of_reviews']\n\nX = X.astype(float)\nY = Y.astype(float)\n\n# Step 4: Fit Poisson regression model\nmodel = sm.GLM(Y, X, family=sm.families.Poisson())\nresults = model.fit()\n\nprint(results.summary())\n```\n\n### Interpreting Model Coefficients: Variation in Number of Reviews\n\nEstimated a Poisson regression model using listing-level data from AirBnB in New York City. The dependent variable is the number of reviews, which I use as a proxy for the number of bookings. The predictors include listing characteristics (price, bedrooms, bathrooms), review scores, and binary indicators for booking features and room types.\n\n#### Model Summary\n\n- **Dependent variable**: `number_of_reviews`\n- **Model family**: Poisson (log link)\n- **Observations**: 30,160\n- **Pseudo R² (Cragg & Uhler's)**: 0.684 — indicates strong explanatory power for count data.\n- **All predictors are statistically significant (p < 0.05)**\n\n#### Key Coefficient Interpretations\n\n#### Baseline (Intercept)\n- `const = 3.4980`\n  - When all predictors are 0, the expected number of reviews is:  \n    $$ \\exp(3.4980) \\approx 33 $$\n\n#### Continuous Predictors\n\n- **`days`**: 0.00005072  \n  - Each additional day listed increases expected reviews by ~0.5% per 100 days.\n\n- **`bathrooms`**: -0.1177  \n  - Each additional bathroom is associated with ~11.1% fewer reviews  \n    $$ \\exp(-0.1177) \\approx 0.889 $$\n\n- **`bedrooms`**: 0.0741  \n  - Each additional bedroom is associated with ~7.7% more reviews  \n    $$ \\exp(0.0741) \\approx 1.077 $$\n\n- **`price`**: -1.791e-5  \n  - Very small negative effect. Each additional dollar reduces expected reviews by a negligible amount.\n\n#### Review Scores\n\n- **`review_scores_cleanliness`**: 0.1131  \n  - Higher cleanliness scores → ~12% more reviews per point increase\n\n- **`review_scores_location`**: -0.0769  \n  - Higher location scores → ~7.4% fewer reviews (unexpected, may indicate multicollinearity)\n\n- **`review_scores_value`**: -0.0911  \n  - Higher value scores → ~8.7% fewer reviews\n\n#### Categorical Predictors (relative to reference category)\n\n- **`room_type_Private room`**: -0.0105  \n  - Private rooms receive ~1% fewer reviews than entire places\n\n- **`room_type_Shared room`**: -0.2463  \n  - Shared rooms receive ~22% fewer reviews  \n    $$ \\exp(-0.2463) \\approx 0.782 $$\n\n- **`instant_bookable_t`**: 0.3459  \n  - Listings that are instantly bookable receive ~41% more reviews  \n    $$ \\exp(0.3459) \\approx 1.413 $$\n\n#### Key Takeaways\n\n- Listings with **instant booking**, more **bedrooms**, and higher **cleanliness scores** tend to receive more reviews.\n- Shared rooms and higher bathroom count are associated with **fewer reviews**.\n- The negative coefficients for `location` and `value` are counterintuitive and may suggest multicollinearity or omitted variable bias.\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.5","theme":"cosmo","title":"Poisson Regression Examples","author":"Alexa Gamble","date":"05/03/2025","callout-appearance":"minimal"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}