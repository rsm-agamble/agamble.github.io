{
  "hash": "7c7c0d392bd9822d3a20cb0f71759129",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Poisson Regression Examples\"\nauthor: \"Alexa Gamble\"\ndate: 05/03/2025\ncallout-appearance: minimal # this hides the blue \"i\" icon on .callout-notes\n---\n\n\n## Blueprinty Case Study HW 2\n\n### Introduction\n\nBlueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. \n\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.\n\n\n### Data\n\n::: {#0dc3bec1 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nblueprinty = pd.read_csv(\"/home/jovyan/Desktop/Marketing Analytics Website : HW's/blueprinty.csv\")\nairbnb = pd.read_csv(\"/home/jovyan/Desktop/Marketing Analytics Website : HW's/airbnb.csv\")\n\nblueprinty\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>patents</th>\n      <th>region</th>\n      <th>age</th>\n      <th>iscustomer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Midwest</td>\n      <td>32.5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>Southwest</td>\n      <td>37.5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>Northwest</td>\n      <td>27.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Northeast</td>\n      <td>24.5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>Southwest</td>\n      <td>37.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1495</th>\n      <td>2</td>\n      <td>Northeast</td>\n      <td>18.5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1496</th>\n      <td>3</td>\n      <td>Southwest</td>\n      <td>22.5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1497</th>\n      <td>4</td>\n      <td>Southwest</td>\n      <td>17.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1498</th>\n      <td>3</td>\n      <td>South</td>\n      <td>29.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1499</th>\n      <td>1</td>\n      <td>South</td>\n      <td>39.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1500 rows × 4 columns</p>\n</div>\n```\n:::\n:::\n\n\n### Compare histograms and means of number of patents by customer status\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n::: {#72562038 .cell execution_count=2}\n``` {.python .cell-code}\ncustomers = blueprinty[blueprinty['iscustomer'] == 1]\nnon_customers = blueprinty[blueprinty['iscustomer'] == 0]\n\nplt.figure(figsize=(10, 5))\nplt.hist(customers['patents'], bins=20, alpha=0.6, label='Customers', edgecolor='black')\nplt.hist(non_customers['patents'], bins=20, alpha=0.6, label='Non-Customers', edgecolor='black')\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histogram of Number of Patents by Customer Status')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nmean_customers = customers['patents'].mean()\nmean_non_customers = non_customers['patents'].mean()\n\nprint(f'Customer Mean Number of Patents: {mean_customers}')\nprint(f'Non-Customer Mean Number of Patents: {mean_non_customers}')\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=816 height=449}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nCustomer Mean Number of Patents: 4.133056133056133\nNon-Customer Mean Number of Patents: 3.4730127576054954\n```\n:::\n:::\n\n\nThe histogram above compares the distribution of patent counts between firms that use Blueprinty software (`iscustomer = 1`) and those that do not (`iscustomer = 0`). A few key patterns emerge:\n\n- The distribution is **right-skewed** for both groups, with the bulk of firms holding between 0 and 6 patents.\n- **Non-customers** are more concentrated around lower patent counts, particularly at 2-4 patents.\n- **Customers** of Blueprinty show a relatively flatter distribution, with more firms holding moderate to higher numbers of patents.\n\nThe summary statistics support this visual trend:\n\n- Non-customers have **3.47 patents** on average (n = 1,019)\n- Customers have **4.13 patents** on average (n = 481)\n\nAlthough this is a modest difference (approximately 0.66 more patents on average), it is consistent with the idea that Blueprinty customers may be more successful or more prolific in obtaining patents. However, it is also possible that other factor, such as firm age or region, may explain the difference, so further analysis is needed before drawing causal conclusions.\n\n\n### Compare regions and ages by customer status\n\n::: {#0ee31af6 .cell execution_count=3}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_style(\"whitegrid\")\n\nregion_props = pd.crosstab(blueprinty['region'], blueprinty['iscustomer'], normalize='index').reset_index()\nregion_props.columns = ['Region', 'Non-Customers', 'Customers']\nregion_props = pd.melt(region_props, id_vars='Region', var_name='Customer Type', value_name='Proportion')\n\nplt.figure(figsize=(8, 5))\nsns.barplot(data=region_props, x='Region', y='Proportion', hue='Customer Type')\nplt.title('Proportion of Firms by Region and Customer Status')\nplt.ylabel('Proportion')\nplt.xlabel('Region')\nplt.legend(title='Customer Type')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=757 height=469}\n:::\n:::\n\n\n::: {#cb5eb50d .cell execution_count=4}\n``` {.python .cell-code}\nplt.figure(figsize=(6, 5))\nsns.boxplot(data=blueprinty, x='iscustomer', y='age')\nplt.title('Age of Firms by Customer Status')\nplt.xlabel('Customer Status (0 = Non-Customer, 1 = Customer)')\nplt.ylabel('Years Since Incorporation')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=565 height=468}\n:::\n:::\n\n\nThe region plot above shows notable variation in customer distribution across regions:\n\n- In most regions (Midwest, Northwest, South, Southwest), the majority of firms are **non-customers**, with Blueprinty customers making up only around 15–20% of firms.\n- The **Northeast** stands out as an exception, where nearly **half of the firms** are Blueprinty customers. This suggests that Blueprinty may have a stronger market presence or better adoption in that region.\n\nThe boxplot comparing firm age by customer status shows:\n\n- The **median age** of customers and non-customers is very similar, both around 25–27 years.\n- The **spread of firm ages** is also comparable, though customers appear to have a slightly higher upper range, with some firms approaching 50 years of age.\n- There’s no strong visual evidence of systematic age differences between customers and non-customers, but customers might skew slightly older on average.\n\nTogether, these plots suggest that while **age** may not be a major confounding factor, **region** could be, especially due to the Northeast's unusual customer concentration. This is important to account for in any causal claims about Blueprinty’s effectiveness.\n\n\n### Estimation of Simple Poisson Model\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, I can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. I start by estimating a simple Poisson model via Maximum Likelihood.\n\n#### Likelihood Function\n\nFor a Poisson model where $Y_i \\sim \\text{Poisson}(\\lambda)$, the probability mass function is:\n\n$$\nf(Y_i | \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n$$\n\nAssuming independence across $n$ observations, the **likelihood** is:\n\n$$\nL(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n$$\n\nThis is then simplified to:\n\n$$\nL(\\lambda) = \\frac{e^{-n\\lambda} \\lambda^{\\sum Y_i}}{\\prod Y_i!}\n$$\n\n::: {#5a44140b .cell execution_count=5}\n``` {.python .cell-code}\nfrom scipy.special import gammaln\n\ndef poisson_log_likelihood(lmbda, y):\n    if lmbda <= 0:\n        return -np.inf\n    return np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1)) \n```\n:::\n\n\n### Plot of Log-Likelihood Across Lambda Values\n\n::: {#d410d998 .cell execution_count=6}\n``` {.python .cell-code}\ny = blueprinty['patents'].values\n\nlambda_vals = np.linspace(0.1, 10, 200)\n\nlog_liks = [poisson_log_likelihood(lmbda, y) for lmbda in lambda_vals]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, log_liks)\nplt.title(\"Log-Likelihood of Poisson Model vs Lambda\")\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=696 height=449}\n:::\n:::\n\n\n### Solving for MLE of Lambda (Analytical Derivation)\n\nTo find the MLE of $\\lambda$, take the log-likelihood:\n\n$$\n\\ell(\\lambda) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right)\n$$\n\nNow take the derivative with respect to $\\lambda$:\n\n$$\n\\frac{d\\ell}{d\\lambda} = \\sum_{i=1}^n \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n= -n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i\n$$\n\nSet the derivative equal to zero and solve:\n\n$$\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = 0\n$$\n\n$$\n\\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = n\n$$\n\n$$\n\\lambda_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n$$\n\nThus, the maximum likelihood estimate of $\\lambda$ is simply the **sample mean** of $Y$, which makes intuitive sense: the Poisson distribution models counts with mean $\\lambda$, so the best estimate of $\\lambda$ is just the average count in the data.\n\n### Find MLE of Lambda via Optimization\n\n::: {#c7eb1c50 .cell execution_count=7}\n``` {.python .cell-code}\nfrom scipy.optimize import minimize_scalar\n\ndef neg_poisson_log_likelihood(lmbda, y):\n    if lmbda <= 0:\n        return np.inf\n    return -np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))\n\ny = blueprinty['patents'].values\n\nresult = minimize_scalar(neg_poisson_log_likelihood, bounds=(0.1, 10), args=(y,), method='bounded')\n\nlambda_mle = result.x\nlambda_mle\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n3.684666226134359\n```\n:::\n:::\n\n\n### Estimation of Poisson Regression Model\n\nNext, extended our simple Poisson model to a Poisson Regression Model such that $Y_i = \\text{Poisson}(\\lambda_i)$ where $\\lambda_i = \\exp(X_i'\\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, I will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n::: {#63cb41c8 .cell execution_count=8}\n``` {.python .cell-code}\nimport math\n\ndef poisson_regression_likelihood(beta, X, y, alpha=0.01):\n    X = np.asarray(X, dtype=float)\n    beta = np.asarray(beta, dtype=float)\n    y = np.asarray(y, dtype=float)\n\n    eta = X @ beta\n    eta = np.clip(eta, -100, 100)\n    lam = np.exp(eta)\n\n    log_likelihood = np.sum(y * eta - lam - gammaln(y + 1))\n    penalty = alpha * np.sum(beta**2)\n    return log_likelihood - penalty\n```\n:::\n\n\n::: {#7d6e9a32 .cell execution_count=9}\n``` {.python .cell-code}\nfrom scipy.optimize import minimize\n\nblueprinty['age_squared'] = blueprinty['age'] ** 2\nregion_dummies = pd.get_dummies(blueprinty['region'], drop_first=True)\nX = pd.concat([\n    pd.Series(1, index=blueprinty.index, name='intercept'),\n    blueprinty[['age', 'age_squared', 'iscustomer']],\n    region_dummies\n], axis=1)\nY = blueprinty['patents'].values\nX_matrix = X.values.astype(float)\n\ndef neg_log_likelihood(beta, X, Y):\n    return -poisson_regression_likelihood(beta, X, Y, alpha=0.01)\n\n# Optimize to find MLE\ninitial_beta = np.zeros(X_matrix.shape[1])\nresult = minimize(neg_log_likelihood, x0=initial_beta, args=(X_matrix, Y), method='BFGS')\n\n# Extract MLE and Hessian inverse\nbeta_mle = result.x\nhess_inv = result.hess_inv\n\nif not isinstance(hess_inv, np.ndarray):\n    hess_inv = hess_inv.todense()\nhess_inv = np.asarray(hess_inv)\n\nstd_errors = np.sqrt(np.diag(hess_inv))\n\nresults_df = pd.DataFrame({\n    \"Coefficient\": beta_mle,\n    \"Std. Error\": std_errors\n}, index=X.columns)\nresults_df\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Coefficient</th>\n      <th>Std. Error</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>intercept</th>\n      <td>-0.509635</td>\n      <td>0.203671</td>\n    </tr>\n    <tr>\n      <th>age</th>\n      <td>0.148680</td>\n      <td>0.015850</td>\n    </tr>\n    <tr>\n      <th>age_squared</th>\n      <td>-0.002972</td>\n      <td>0.000291</td>\n    </tr>\n    <tr>\n      <th>iscustomer</th>\n      <td>0.207599</td>\n      <td>0.027778</td>\n    </tr>\n    <tr>\n      <th>Northeast</th>\n      <td>0.029144</td>\n      <td>0.040528</td>\n    </tr>\n    <tr>\n      <th>Northwest</th>\n      <td>-0.017593</td>\n      <td>0.049131</td>\n    </tr>\n    <tr>\n      <th>South</th>\n      <td>0.056546</td>\n      <td>0.052015</td>\n    </tr>\n    <tr>\n      <th>Southwest</th>\n      <td>0.050551</td>\n      <td>0.042465</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#30b4bba1 .cell execution_count=10}\n``` {.python .cell-code}\nimport statsmodels.api as sm\n\nX_sm = sm.add_constant(X_matrix, has_constant='add')\ny_sm = Y\n\n# Fit Poisson GLM\nmodel = sm.GLM(y_sm, X_sm, family=sm.families.Poisson())\nresults = model.fit()\n\nprint(results.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Wed, 07 May 2025   Deviance:                       2143.3\nTime:                        19:08:37   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.2545      0.092     -2.778      0.005      -0.434      -0.075\nx1            -0.2545      0.092     -2.778      0.005      -0.434      -0.075\nx2             0.1486      0.014     10.716      0.000       0.121       0.176\nx3            -0.0030      0.000    -11.513      0.000      -0.003      -0.002\nx4             0.2076      0.031      6.719      0.000       0.147       0.268\nx5             0.0292      0.044      0.669      0.504      -0.056       0.115\nx6            -0.0176      0.054     -0.327      0.744      -0.123       0.088\nx7             0.0566      0.053      1.074      0.283      -0.047       0.160\nx8             0.0506      0.047      1.072      0.284      -0.042       0.143\n==============================================================================\n```\n:::\n:::\n\n\n#### Results and Interpretation\n\nEstimated a Poisson regression model to examine the factors influencing the number of patents. The model includes predictors for age (both linear and squared), customer status, and region.\n\n**Key findings:**\n\n- **Age**  \n  The coefficient for age is `0.1487`, which is statistically significant (*p* < 0.01). Interpreted on the log scale, this suggests that each additional year of age increases the expected number of patents by approximately **16%** (`exp(0.1487) ≈ 1.16`), holding other variables constant.\n\n- **Age squared**  \n  The coefficient for age squared is `-0.0030` and statistically significant. This negative value indicates a **diminishing marginal return to age**—the effect of age on patent count decreases as individuals grow older.\n\n- **Customer status**  \n  The coefficient for `iscustomer` is `0.2076`, meaning customers are expected to have approximately **23% more patents** than non-customers (`exp(0.2076) ≈ 1.23`). This effect is also statistically significant.\n\n- **Region**  \n  The regional dummy variables (Northeast, Northwest, South, Southwest) are **not statistically significant** (*p* > 0.05), suggesting that once age and customer status are accounted for, geographic region does not have a strong impact on patenting activity.\n\n- **Model fit**  \n  - Log-Likelihood: `-3258.1`  \n  - Pseudo R-squared (Cragg-Uhler): `0.136`  \n  - Deviance: `2143.3`  \n  - Pearson chi-squared: `2070`\n\n  These metrics suggest a **modest fit**. The relatively high Pearson chi-squared statistic hints at potential overdispersion, which may justify trying a Negative Binomial model in future work.\n\n**Conclusion:**  \nAge and customer status are strong and statistically significant predictors of patent activity. The relationship with age is nonlinear, and customers tend to file more patents than non-customers. In contrast, regional effects are weak and not statistically meaningful in this model.\n\n\n\n#### Estimated Effect of Blueprinty's Software on Patent Counts\n\n::: {#ed81b8a0 .cell execution_count=11}\n``` {.python .cell-code}\nX_0 = X_matrix.copy()\nX_1 = X_matrix.copy()\n\niscustomer_index = X.columns.get_loc('iscustomer') \n\nX_0[:, iscustomer_index] = 0\nX_1[:, iscustomer_index] = 1\n\nbeta_hat = result.x\n\neta_0 = X_0 @ beta_hat\neta_1 = X_1 @ beta_hat\n\neta_0 = np.clip(eta_0, -100, 100) \neta_1 = np.clip(eta_1, -100, 100)\n\ny_pred_0 = np.exp(eta_0)\ny_pred_1 = np.exp(eta_1)\n\ndelta = y_pred_1 - y_pred_0\naverage_effect = np.mean(delta)\n\nprint(f\"Average effect of Blueprinty’s software: {average_effect:.4f} more patents per firm\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAverage effect of Blueprinty’s software: 0.7928 more patents per firm\n```\n:::\n:::\n\n\nTo estimate the causal impact of using Blueprinty's software on patenting activity, simulated two counterfactual scenarios:\n\n- `X_0`: All firms are set as non-customers (`iscustomer = 0`)\n- `X_1`: All firms are set as customers (`iscustomer = 1`)\n\nUsing our fitted Poisson regression model, predicted the expected number of patents under each scenario and computed the average difference across all firms.\n\n**Result**: On average, firms using Blueprinty's software are predicted to file **0.7928 more patents** than those who do not, holding all other variables constant.\n\nThis suggests a positive and meaningful effect of the software on innovation outcomes.\n\n\n\n## AirBnB Case Study\n\n### Introduction\n\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:\n\n:::: {.callout-note collapse=\"true\"}\n### Variable Definitions\n\n    - `id` = unique ID number for each unit\n    - `last_scraped` = date when information scraped\n    - `host_since` = date when host first listed the unit on Airbnb\n    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n    - `room_type` = Entire home/apt., Private room, or Shared room\n    - `bathrooms` = number of bathrooms\n    - `bedrooms` = number of bedrooms\n    - `price` = price per night (dollars)\n    - `number_of_reviews` = number of reviews for the unit on Airbnb\n    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n    - `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n    - `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n    - `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n::::\n\n### Data Cleaning and Preparation\n\n::: {#ed0122e7 .cell execution_count=12}\n``` {.python .cell-code}\ncolumns_needed = [\n    \"number_of_reviews\", \"days\", \"room_type\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n    \"instant_bookable\"\n]\nairbnb = airbnb[columns_needed].dropna()\n\n# Convert 'instant_bookable' to binary\nairbnb[\"instant_bookable\"] = (airbnb[\"instant_bookable\"] == \"t\").astype(int)\n\n# One-hot encode 'room_type' (drop first category)\nairbnb = pd.get_dummies(airbnb, columns=[\"room_type\"], drop_first=True)\n```\n:::\n\n\n### Explatory Data Analysis \n\n::: {#e4b23aa8 .cell execution_count=13}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Histogram of reviews\nsns.histplot(airbnb[\"number_of_reviews\"], bins=50)\nplt.title(\"Distribution of Number of Reviews\")\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Count\")\nplt.show()\n\n# Reviews by room type\nsns.boxplot(x=\"room_type_Private room\", y=\"number_of_reviews\", data=airbnb)\nplt.title(\"Reviews by Room Type (Private Room Indicator)\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-1.png){width=610 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-2.png){width=593 height=449}\n:::\n:::\n\n\n### Poisson Regression Model\n\n::: {#e8588d31 .cell execution_count=14}\n``` {.python .cell-code}\nimport statsmodels.api as sm\n\nX = airbnb.drop(columns=[\"number_of_reviews\"])\nX = sm.add_constant(X)\nX = X.astype(float)\n\ny = airbnb[\"number_of_reviews\"]\n\nmodel = sm.GLM(y, X, family=sm.families.Poisson())\nresults = model.fit()\n\nprint(results.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:      number_of_reviews   No. Observations:                30160\nModel:                            GLM   Df Residuals:                    30149\nModel Family:                 Poisson   Df Model:                           10\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:            -5.2418e+05\nDate:                Wed, 07 May 2025   Deviance:                   9.2689e+05\nTime:                        19:08:37   Pearson chi2:                 1.37e+06\nNo. Iterations:                    10   Pseudo R-squ. (CS):             0.6840\nCovariance Type:            nonrobust                                         \n=============================================================================================\n                                coef    std err          z      P>|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------------\nconst                         3.4980      0.016    217.396      0.000       3.467       3.530\ndays                       5.072e-05   3.91e-07    129.755      0.000       5e-05    5.15e-05\nbathrooms                    -0.1177      0.004    -31.394      0.000      -0.125      -0.110\nbedrooms                      0.0741      0.002     37.197      0.000       0.070       0.078\nprice                     -1.791e-05   8.33e-06     -2.151      0.031   -3.42e-05   -1.59e-06\nreview_scores_cleanliness     0.1131      0.001     75.611      0.000       0.110       0.116\nreview_scores_location       -0.0769      0.002    -47.796      0.000      -0.080      -0.074\nreview_scores_value          -0.0911      0.002    -50.490      0.000      -0.095      -0.088\ninstant_bookable              0.3459      0.003    119.666      0.000       0.340       0.352\nroom_type_Private room       -0.0105      0.003     -3.847      0.000      -0.016      -0.005\nroom_type_Shared room        -0.2463      0.009    -28.578      0.000      -0.263      -0.229\n=============================================================================================\n```\n:::\n:::\n\n\n::: {#ec22a6c7 .cell execution_count=15}\n``` {.python .cell-code}\nbeta_mle = results.params.values\nstd_errors = results.bse.values\n\npredictor_names = results.model.exog_names\n\nresults_df = pd.DataFrame({\n    \"Coefficient\": beta_mle,\n    \"Std. Error\": std_errors\n}, index=predictor_names)\n\nresults_df = results_df.round(4)\n\nresults_df\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Coefficient</th>\n      <th>Std. Error</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>const</th>\n      <td>3.4980</td>\n      <td>0.0161</td>\n    </tr>\n    <tr>\n      <th>days</th>\n      <td>0.0001</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>bathrooms</th>\n      <td>-0.1177</td>\n      <td>0.0037</td>\n    </tr>\n    <tr>\n      <th>bedrooms</th>\n      <td>0.0741</td>\n      <td>0.0020</td>\n    </tr>\n    <tr>\n      <th>price</th>\n      <td>-0.0000</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>review_scores_cleanliness</th>\n      <td>0.1131</td>\n      <td>0.0015</td>\n    </tr>\n    <tr>\n      <th>review_scores_location</th>\n      <td>-0.0769</td>\n      <td>0.0016</td>\n    </tr>\n    <tr>\n      <th>review_scores_value</th>\n      <td>-0.0911</td>\n      <td>0.0018</td>\n    </tr>\n    <tr>\n      <th>instant_bookable</th>\n      <td>0.3459</td>\n      <td>0.0029</td>\n    </tr>\n    <tr>\n      <th>room_type_Private room</th>\n      <td>-0.0105</td>\n      <td>0.0027</td>\n    </tr>\n    <tr>\n      <th>room_type_Shared room</th>\n      <td>-0.2463</td>\n      <td>0.0086</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Interpreting Model Coefficients: Variation in Number of Reviews\n\nEstimated a Poisson regression model using listing-level data from AirBnB in New York City. The dependent variable is the number of reviews, which I use as a proxy for the number of bookings. The predictors include listing characteristics (price, bedrooms, bathrooms), review scores, and binary indicators for booking features and room types.\n\n**Key coefficient interpretations:**\n\n- **Intercept (`const`) = 3.498**  \n  This is the baseline log expected number of reviews for a listing with all numeric features at zero and the reference categories for categorical variables. While not directly interpretable, it anchors the model.\n\n- **Days active (`days`) = 0.00005**  \n  Each additional day a listing has been active increases the expected number of reviews by about **0.005%** (`exp(0.0000507) ≈ 1.00005`), holding all other variables constant.\n\n- **Bathrooms = -0.1177**  \n  Listings with more bathrooms are associated with **~11% fewer reviews** per additional bathroom (`exp(-0.1177) ≈ 0.889`). This may reflect less frequently booked luxury or multi-bathroom units.\n\n- **Bedrooms = 0.0741**  \n  Each additional bedroom corresponds to an **increase of about 7.7%** in the expected number of reviews (`exp(0.0741) ≈ 1.077`).\n\n- **Price = -0.000018**  \n  A $1 increase in nightly price is associated with a very slight **decrease in expected reviews**, though the effect is economically negligible. This suggests price sensitivity is limited or nonlinear.\n\n- **Review scores:**\n  - **Cleanliness = 0.1131**  \n    Listings rated 1 point higher on cleanliness receive **~12% more reviews** (`exp(0.1131) ≈ 1.12`).\n  - **Location = -0.0769**  \n    Surprisingly, higher location scores are associated with **~7.4% fewer reviews**, possibly due to multicollinearity or non-monotonic guest expectations.\n  - **Value = -0.0911**  \n    A higher value score is associated with **~8.7% fewer reviews**, which again may reflect a complex relationship between ratings and review volume.\n\n- **Instant bookable = 0.3459**  \n  Listings that support instant booking receive **~41% more reviews** on average (`exp(0.3459) ≈ 1.41`), highlighting the importance of convenience in booking behavior.\n\n- **Room type (reference: Entire home/apt):**\n  - **Private room = -0.0105**  \n    Slightly fewer reviews, about **1% less** than entire homes.\n  - **Shared room = -0.2463**  \n    Significantly fewer reviews, with **~22% less activity** compared to entire homes.\n\nOverall, listing convenience (e.g., instant bookable), cleanliness, and size (bedrooms) are strongly associated with greater booking activity. Room type and days listed also matter, while the effect of price and some review scores are more subtle.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}