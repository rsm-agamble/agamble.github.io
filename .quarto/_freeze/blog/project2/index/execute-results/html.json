{
  "hash": "12742ceea61623eeff14a6c4eb1565df",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Poisson Regression Examples\"\nauthor: \"Your Name\"\ndate: today\ncallout-appearance: minimal # this hides the blue \"i\" icon on .callout-notes\n---\n\n\n## Blueprinty Case Study HW 2\n\n### Introduction\n\nBlueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. \n\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.\n\n\n### Data\n\n::: {#70791fd6 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nblueprinty = pd.read_csv(\"/home/jovyan/Desktop/Marketing Analytics Website : HW's/blueprinty.csv\")\nairbnb = pd.read_csv(\"/home/jovyan/Desktop/Marketing Analytics Website : HW's/airbnb.csv\")\n\nblueprinty\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>patents</th>\n      <th>region</th>\n      <th>age</th>\n      <th>iscustomer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Midwest</td>\n      <td>32.5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>Southwest</td>\n      <td>37.5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>Northwest</td>\n      <td>27.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Northeast</td>\n      <td>24.5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>Southwest</td>\n      <td>37.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1495</th>\n      <td>2</td>\n      <td>Northeast</td>\n      <td>18.5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1496</th>\n      <td>3</td>\n      <td>Southwest</td>\n      <td>22.5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1497</th>\n      <td>4</td>\n      <td>Southwest</td>\n      <td>17.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1498</th>\n      <td>3</td>\n      <td>South</td>\n      <td>29.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1499</th>\n      <td>1</td>\n      <td>South</td>\n      <td>39.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1500 rows × 4 columns</p>\n</div>\n```\n:::\n:::\n\n\n### Compare histograms and means of number of patents by customer status\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n::: {#4f1aab87 .cell execution_count=2}\n``` {.python .cell-code}\n# Histogram of number of patents by customer status\n\n# Separate data by customer status\ncustomers = blueprinty[blueprinty['iscustomer'] == 1]\nnon_customers = blueprinty[blueprinty['iscustomer'] == 0]\n\n# Plot histograms of number of patents\nplt.figure(figsize=(10, 5))\nplt.hist(customers['patents'], bins=20, alpha=0.6, label='Customers', edgecolor='black')\nplt.hist(non_customers['patents'], bins=20, alpha=0.6, label='Non-Customers', edgecolor='black')\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histogram of Number of Patents by Customer Status')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Calculate and compare means\nmean_customers = customers['patents'].mean()\nmean_non_customers = non_customers['patents'].mean()\n\nprint(f'Customer Mean Number of Patents: {mean_customers}')\nprint(f'Non-Customer Mean Number of Patents: {mean_non_customers}')\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=816 height=449}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nCustomer Mean Number of Patents: 4.133056133056133\nNon-Customer Mean Number of Patents: 3.4730127576054954\n```\n:::\n:::\n\n\nThe histogram above compares the distribution of patent counts between firms that use Blueprinty software (`iscustomer = 1`) and those that do not (`iscustomer = 0`). A few key patterns emerge:\n\n- The distribution is **right-skewed** for both groups, with the bulk of firms holding between 0 and 6 patents.\n- **Non-customers** are more concentrated around lower patent counts, particularly at 2-4 patents.\n- **Customers** of Blueprinty show a relatively flatter distribution, with more firms holding moderate to higher numbers of patents.\n\nThe summary statistics support this visual trend:\n\n- Non-customers have **3.47 patents** on average (n = 1,019)\n- Customers have **4.13 patents** on average (n = 481)\n\nAlthough this is a modest difference (approximately 0.66 more patents on average), it is consistent with the idea that Blueprinty customers may be more successful or more prolific in obtaining patents. However, it is also possible that other factor, such as firm age or region, may explain the difference, so further analysis is needed before drawing causal conclusions.\n\n\n### Compare regions and ages by customer status\n\n### Compare regions and ages by customer status\n\n::: {#bee7a058 .cell execution_count=3}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_style(\"whitegrid\")\n\nregion_props = pd.crosstab(blueprinty['region'], blueprinty['iscustomer'], normalize='index').reset_index()\nregion_props.columns = ['Region', 'Non-Customers', 'Customers']\nregion_props = pd.melt(region_props, id_vars='Region', var_name='Customer Type', value_name='Proportion')\n\nplt.figure(figsize=(8, 5))\nsns.barplot(data=region_props, x='Region', y='Proportion', hue='Customer Type')\nplt.title('Proportion of Firms by Region and Customer Status')\nplt.ylabel('Proportion')\nplt.xlabel('Region')\nplt.legend(title='Customer Type')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=757 height=469}\n:::\n:::\n\n\n::: {#c15bb328 .cell execution_count=4}\n``` {.python .cell-code}\nplt.figure(figsize=(6, 5))\nsns.boxplot(data=blueprinty, x='iscustomer', y='age')\nplt.title('Age of Firms by Customer Status')\nplt.xlabel('Customer Status (0 = Non-Customer, 1 = Customer)')\nplt.ylabel('Years Since Incorporation')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=565 height=468}\n:::\n:::\n\n\nThe region plot above shows notable variation in customer distribution across regions:\n\n- In most regions (Midwest, Northwest, South, Southwest), the majority of firms are **non-customers**, with Blueprinty customers making up only around 15–20% of firms.\n- The **Northeast** stands out as an exception, where nearly **half of the firms** are Blueprinty customers. This suggests that Blueprinty may have a stronger market presence or better adoption in that region.\n\nThe boxplot comparing firm age by customer status shows:\n\n- The **median age** of customers and non-customers is very similar, both around 25–27 years.\n- The **spread of firm ages** is also comparable, though customers appear to have a slightly higher upper range, with some firms approaching 50 years of age.\n- There’s no strong visual evidence of systematic age differences between customers and non-customers, but customers might skew slightly older on average.\n\nTogether, these plots suggest that while **age** may not be a major confounding factor, **region** could be, especially due to the Northeast's unusual customer concentration. This is important to account for in any causal claims about Blueprinty’s effectiveness.\n\n\n### Estimation of Simple Poisson Model\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n#### Likelihood Function\n\nFor a Poisson model where $Y_i \\sim \\text{Poisson}(\\lambda)$, the probability mass function is:\n\n$$\nf(Y_i | \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n$$\n\nAssuming independence across $n$ observations, the **likelihood** is:\n\n$$\nL(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n$$\n\nTaking the natural log of the likelihood (log-likelihood):\n\n$$\n\\ell(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right)\n$$\n\n::: {#3f38c69f .cell execution_count=5}\n``` {.python .cell-code}\nfrom scipy.special import gammaln\n\ndef poisson_log_likelihood(lmbda, y):\n    if lmbda <= 0:\n        return -np.inf\n    return np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))  # log(y!) = gammaln(y + 1)\n```\n:::\n\n\n### Plot of Log-Likelihood Across Lambda Values\n\n::: {#5777e469 .cell execution_count=6}\n``` {.python .cell-code}\nimport numpy as np\n\ny = blueprinty['patents'].values\n\n# Range of lambda values to test\nlambda_vals = np.linspace(0.1, 10, 200)\n\nlog_liks = [poisson_log_likelihood(lmbda, y) for lmbda in lambda_vals]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, log_liks)\nplt.title(\"Log-Likelihood of Poisson Model vs Lambda\")\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=696 height=449}\n:::\n:::\n\n\n### Solving for MLE of Lambda (Analytical Derivation)\n\nTo find the MLE of $\\lambda$, we take the log-likelihood:\n\n$$\n\\ell(\\lambda) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right)\n$$\n\nNow take the derivative with respect to $\\lambda$:\n\n$$\n\\frac{d\\ell}{d\\lambda} = \\sum_{i=1}^n \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n= -n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i\n$$\n\nSet the derivative equal to zero and solve:\n\n$$\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = 0\n$$\n\n$$\n\\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = n\n$$\n\n$$\n\\lambda_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n$$\n\nThus, the maximum likelihood estimate of $\\lambda$ is simply the **sample mean** of $Y$, which makes intuitive sense: the Poisson distribution models counts with mean $\\lambda$, so the best estimate of $\\lambda$ is just the average count in the data.\n\n_todo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python._\n\n### Find MLE of Lambda via Optimization\n\n::: {#52ee384f .cell execution_count=7}\n``` {.python .cell-code}\nfrom scipy.optimize import minimize_scalar\nfrom scipy.special import gammaln\nimport numpy as np\n\n# Define negative log-likelihood for minimization\ndef neg_poisson_log_likelihood(lmbda, y):\n    if lmbda <= 0:\n        return np.inf\n    return -np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))\n\ny = blueprinty['patents'].values\n\n# Optimize (minimize the negative log-likelihood)\nresult = minimize_scalar(neg_poisson_log_likelihood, bounds=(0.1, 10), args=(y,), method='bounded')\n\nlambda_mle = result.x\nlambda_mle\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n3.684666226134359\n```\n:::\n:::\n\n\n### Estimation of Poisson Regression Model\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \\text{Poisson}(\\lambda_i)$ where $\\lambda_i = \\exp(X_i'\\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n#### Poisson Regression Log-Likelihood Function\n\n::: {#208a52be .cell execution_count=8}\n``` {.python .cell-code}\nimport numpy as np\nfrom scipy.special import gammaln\n\n# Log-likelihood for Poisson regression\ndef poisson_regression_log_likelihood(beta, X, y):\n    beta = np.asarray(beta)\n    X = np.asarray(X)\n    y = np.asarray(y)\n    \n    # Linear predictor and lambda_i = exp(X @ beta)\n    eta = X @ beta\n    lam = np.exp(eta)\n\n    # Log-likelihood\n    log_lik = np.sum(-lam + y * eta - gammaln(y + 1))\n    return -log_lik\n```\n:::\n\n\n_todo: Use your function along with R's optim() or Python's sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1's to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors._\n\n#### Build Design Matrix for Poisson Regression\n\n::: {#ee71e713 .cell execution_count=9}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\n\n# Create design matrix X\nblueprinty['age_squared'] = blueprinty['age']**2\n\n# Get region dummies, dropping one (e.g., alphabetically first)\nregion_dummies = pd.get_dummies(blueprinty['region'], drop_first=True)\n\n# Combine all predictors\nX = pd.concat([\n    pd.Series(1, index=blueprinty.index, name='intercept'),\n    blueprinty[['age', 'age_squared', 'iscustomer']],\n    region_dummies\n], axis=1)\n\ny = blueprinty['patents'].values\nX_matrix = X.values\n```\n:::\n\n\n#### Estimate Beta via Maximum Likelihood (Fixed)\n\n::: {#23f9f804 .cell execution_count=10}\n``` {.python .cell-code}\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\nimport numpy as np\n\n# Ensure X and y are numeric arrays\nX_matrix = np.asarray(X.values, dtype=np.float64)\ny = np.asarray(blueprinty['patents'].values, dtype=np.float64)\n\n# Log-likelihood for Poisson regression (negated for minimization)\ndef poisson_regression_log_likelihood(beta, X, y):\n    beta = np.asarray(beta, dtype=np.float64)\n    eta = X @ beta\n    lam = np.exp(eta)  # ensure eta is a NumPy array\n    return -np.sum(-lam + y * eta - gammaln(y + 1))\n\n# Initial guess\nbeta_init = np.zeros(X_matrix.shape[1])\n\n# Minimize\nresult = minimize(\n    poisson_regression_log_likelihood,\n    beta_init,\n    args=(X_matrix, y),\n    method='BFGS'\n)\n\n# Estimated coefficients\nbeta_hat = result.x\nbeta_hat\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_98000/2408363688.py:13: RuntimeWarning:\n\noverflow encountered in exp\n\n/opt/conda/lib/python3.12/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning:\n\noverflow encountered in reduce\n\n/opt/conda/lib/python3.12/site-packages/scipy/optimize/_numdiff.py:590: RuntimeWarning:\n\ninvalid value encountered in subtract\n\n/tmp/ipykernel_98000/2408363688.py:13: RuntimeWarning:\n\noverflow encountered in exp\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\narray([0., 0., 0., 0., 0., 0., 0., 0.])\n```\n:::\n:::\n\n\n#### Extract Standard Errors and Summarize Results\n\n::: {#5065befe .cell execution_count=11}\n``` {.python .cell-code}\n# Hessian inverse = estimated variance-covariance matrix\nvcov = result.hess_inv\nstandard_errors = np.sqrt(np.diag(vcov))\n\n# Combine into summary table\nsummary_table = pd.DataFrame({\n    'Coefficient': beta_hat,\n    'Std. Error': standard_errors\n}, index=X.columns)\n\nsummary_table\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Coefficient</th>\n      <th>Std. Error</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>intercept</th>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>age</th>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>age_squared</th>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>iscustomer</th>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Northeast</th>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Northwest</th>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>South</th>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>Southwest</th>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n_todo: Check your results using R's glm() function or Python sm.GLM() function._\n\n_todo: Interpret the results._ \n\n_todo: What do you conclude about the effect of Blueprinty's software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences._\n\n\n\n\n## AirBnB Case Study\n\n### Introduction\n\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:\n\n:::: {.callout-note collapse=\"true\"}\n### Variable Definitions\n\n    - `id` = unique ID number for each unit\n    - `last_scraped` = date when information scraped\n    - `host_since` = date when host first listed the unit on Airbnb\n    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n    - `room_type` = Entire home/apt., Private room, or Shared room\n    - `bathrooms` = number of bathrooms\n    - `bedrooms` = number of bedrooms\n    - `price` = price per night (dollars)\n    - `number_of_reviews` = number of reviews for the unit on Airbnb\n    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n    - `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n    - `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n    - `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n::::\n\n\n_todo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided._\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}