[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\nYour Name\nMay 3, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/project1/index.html",
    "href": "blog/project1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis large-scale natural field experiment was designed to explore whether price incentives, framed through matching gifts, influence charitable giving behavior. In the treatment group, donors were told that a “concerned fellow member” would match their donation at varying rates: 1:1, 2:1, or 3:1. Each donor was randomly assigned to one of these match ratios, to a specific suggested donation amount, and to a maximum gift match cap. In contrast, the control group received a standard appeal letter without any mention of a match.\nThe study investigates how these price frames affect both the likelihood of giving and the amount donated. Importantly, the match offers were real and implemented through a conditional agreement with anonymous donors. The paper contributes to the economic literature by testing whether traditional assumptions about donor price sensitivity hold in real-world charitable giving, an area long theorized but rarely tested at this scale and with this level of experimental control.\nThis report replicates and extends aspects of Karlan and List’s analysis using their public dataset. Specifically, I analyze whether matched donations lead to higher response rates, whether larger match ratios produce stronger effects, and whether the treatment influenced the size of contributions. I also use simulation to illustrate key statistical concepts such as the Law of Large Numbers and the Central Limit Theorem, which underpin the validity of the experimental estimates.\nThis project seeks to replicate their results.\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\ndf_k = pd.read_stata(\"/home/jovyan/Desktop/Marketing Analytics Website : HW's/karlan_list_2007.dta\")\ndf_k\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.000000\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.000000\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.000000\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n50078\n1\n0\n1\n0\n0\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.872797\n0.089959\n0.257265\n2.13\n45047.0\n0.771316\n0.263744\n1.000000\n\n\n50079\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.688262\n0.108889\n0.288792\n2.67\n74655.0\n0.741931\n0.586466\n1.000000\n\n\n50080\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\n0.900000\n0.021311\n0.178689\n2.36\n26667.0\n0.778689\n0.107930\n0.000000\n\n\n50081\n1\n0\n3\n0\n1\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.917206\n0.008257\n0.225619\n2.57\n39530.0\n0.733988\n0.184768\n0.634903\n\n\n50082\n1\n0\n3\n0\n1\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.530023\n0.074112\n0.340698\n3.70\n48744.0\n0.717843\n0.127941\n0.994181\n\n\n\n\n50083 rows × 51 columns\n\n\n\nThe dataset consists of 50,083 observations and 51 variables, and appears to originate from a field experiment or observational study, likely related to donation behavior or outreach strategies. Each row represents an individual unit (e.g., a household or donor), and the dataset includes both treatment assignments and a range of covariates.\n\n\n\ntreatment, control: Binary indicators denoting experimental group assignment. A value of 1 in treatment indicates the subject received the treatment condition, while 1 in control indicates assignment to the control group. These two variables are mutually exclusive.\nratio, ratio2, ratio3: Variables reflecting matching incentives (e.g., 1:1, 2:1 matches) provided during the intervention. These are likely categorical or indicator variables showing the ratio level applied.\nsize, size25, size50, size100, sizeno: Donation ask size variables. The size column includes textual representations of the suggested donation amount (e.g., \"$100,000\"), while the other columns serve as binary indicators for specific size categories.\n\n\n\n\n\nredcty, bluecty: Indicators of political affiliation or voting patterns in the subject’s county or city.\npwhite, pblack: Proportion of white and Black residents in the geographic area.\npage18_39: Proportion of the population between ages 18 and 39.\nave_hh_sz: Average household size.\nmedian_hhincome: Median household income in the area.\npowner: Proportion of homeowners.\npsch_atlstba: Proportion of the population with at least a bachelor’s degree.\npop_propurban: Proportion of the population living in urban areas.\n\n\n\n\nSeveral rows contain missing values (NaN), especially in demographic variables like pwhite, pblack, and median_hhincome. These may result from incomplete geographic data or limitations in census coverage.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\n\n\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nTo assess whether the random assignment mechanism was successful, I conduct balance tests on three baseline variables that are not influenced by treatment:\n\nmrm2: months since last donation\n\npwhite: proportion of white residents in the donor’s area\n\nave_hh_sz: average household size\n\nFor each variable, I apply both a manual t-test and a linear regression to compare treatment and control groups. These methods should yield consistent results, since both estimate the same underlying difference in means.\n\nimport statsmodels.formula.api as smf\nfrom scipy import stats\n\n# Function to compute manual t-test and regression for a given variable\ndef balance_test(df, variable, treatment_col=\"treatment\"):\n    df_clean = df.dropna(subset=[variable, treatment_col])\n    treat = df_clean[df_clean[treatment_col] == 1][variable]\n    control = df_clean[df_clean[treatment_col] == 0][variable]\n\n    # Manual t-test\n    mean_treat = treat.mean()\n    mean_control = control.mean()\n    var_treat = treat.var(ddof=1)\n    var_control = control.var(ddof=1)\n    n_treat = len(treat)\n    n_control = len(control)\n    se = np.sqrt(var_treat/n_treat + var_control/n_control)\n    t_stat = (mean_treat - mean_control) / se\n\n    # Welch's degrees of freedom\n    df_num = (var_treat/n_treat + var_control/n_control)**2\n    df_denom = ((var_treat/n_treat)**2 / (n_treat - 1)) + ((var_control/n_control)**2 / (n_control - 1))\n    df_eff = df_num / df_denom\n\n    print(f\"\\n=== Balance Test: {variable} ===\")\n    print(f\"Manual t-test: t = {t_stat:.4f}, df ≈ {df_eff:.2f}\")\n\n    # Linear regression\n    formula = f\"{variable} ~ {treatment_col}\"\n    model = smf.ols(formula, data=df_clean).fit()\n    coef = model.params[treatment_col]\n    pval = model.pvalues[treatment_col]\n    print(f\"Regression coefficient on treatment: {coef:.4f}, p = {pval:.4f}\")\n    return model.summary()\n\nbalance_test(df_k, \"mrm2\")\nbalance_test(df_k, \"pwhite\")\nbalance_test(df_k, \"ave_hh_sz\")\n\n\n=== Balance Test: mrm2 ===\nManual t-test: t = 0.1195, df ≈ 33394.48\nRegression coefficient on treatment: 0.0137, p = 0.9049\n\n=== Balance Test: pwhite ===\nManual t-test: t = -0.5590, df ≈ 31876.22\nRegression coefficient on treatment: -0.0009, p = 0.5753\n\n=== Balance Test: ave_hh_sz ===\nManual t-test: t = 0.8234, df ≈ 31960.02\nRegression coefficient on treatment: 0.0030, p = 0.4098\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nave_hh_sz\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.6794\n\n\nDate:\nSat, 03 May 2025\nProb (F-statistic):\n0.410\n\n\nTime:\n12:44:07\nLog-Likelihood:\n-21524.\n\n\nNo. Observations:\n48221\nAIC:\n4.305e+04\n\n\nDf Residuals:\n48219\nBIC:\n4.307e+04\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n2.4270\n0.003\n812.995\n0.000\n2.421\n2.433\n\n\ntreatment\n0.0030\n0.004\n0.824\n0.410\n-0.004\n0.010\n\n\n\n\n\n\n\n\nOmnibus:\n1488.463\nDurbin-Watson:\n2.003\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n4066.993\n\n\nSkew:\n0.007\nProb(JB):\n0.00\n\n\nKurtosis:\n4.423\nCond. No.\n3.23\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nManual t-test:\n\nt-statistic: 0.1195, degrees of freedom ≈ 33,394, p = 0.905\nNot statistically significant\n\nRegression (mrm2 ~ treatment):\n\nCoefficient on treatment: 0.0137\np-value: 0.905\n\n\nInterpretation: There is no statistically significant difference in prior giving recency between the groups, indicating balance.\n\n\n\n\n\nManual t-test:\n\nt-statistic: –0.5590, degrees of freedom ≈ 31,876, p = 0.5753\nNot statistically significant\n\nRegression (pwhite ~ treatment):\n\nCoefficient on treatment: –0.0009\np-value: 0.5753\n\n\nInterpretation: Racial composition across treatment and control groups is nearly identical, with no meaningful difference detected.\n\n\n\n\n\nManual t-test:\n\nt-statistic: 0.8234, degrees of freedom ≈ 31,960, p = 0.4108\nNot statistically significant\n\nRegression (ave_hh_sz ~ treatment):\n\nCoefficient on treatment: 0.0030\np-value: 0.4108\n\n\nInterpretation: Household size is well balanced between the groups, with the regression confirming no significant difference.\n\n\n\n\nThe results from all three variables show no statistically significant differences between the treatment and control groups. This supports the integrity of the randomization process and aligns with Table 1 in Karlan and List (2007), which was included in the original paper for the same purpose.\nBy demonstrating that key baseline characteristics are similar across groups, we can be more confident that any observed differences in outcome variables can be attributed to the treatment itself rather than to underlying differences in the populations.\nThe balance tests confirm that random assignment was successfully implemented in the experiment.\n\n\n\n\n\n\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nimport matplotlib.pyplot as plt\n\ndf_k['gave'] = (df_k['amount'] &gt; 0).astype(int)\n\nresponse_rates = df_k.groupby('treatment')['gave'].mean()\nlabels = ['Control', 'Treatment']\n\nplt.figure(figsize=(6, 4))\nplt.bar(labels, response_rates, color=['gray', 'blue'])\nplt.ylabel('Proportion Donated')\nplt.title('Response Rate by Group')\nplt.ylim(0, 0.05) \nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ndf_k['gave'] = (df_k['amount'] &gt; 0).astype(int)\n\ngave_treat = df_k[df_k['treatment'] == 1]['gave']\ngave_control = df_k[df_k['treatment'] == 0]['gave']\n\n# Manual t-test (Welch's t-test)\nmean_treat = gave_treat.mean()\nmean_control = gave_control.mean()\nvar_treat = gave_treat.var(ddof=1)\nvar_control = gave_control.var(ddof=1)\nn_treat = len(gave_treat)\nn_control = len(gave_control)\n\nse = np.sqrt(var_treat/n_treat + var_control/n_control)\nt_stat = (mean_treat - mean_control) / se\n\ndf_num = (var_treat/n_treat + var_control/n_control)**2\ndf_denom = ((var_treat/n_treat)**2 / (n_treat - 1)) + ((var_control/n_control)**2 / (n_control - 1))\ndf_eff = df_num / df_denom\n\np_val = 2 * (1 - stats.t.cdf(abs(t_stat), df=df_eff))\n\nprint(f\"T-test result: t = {t_stat:.4f}, df ≈ {df_eff:.0f}, p = {p_val:.4f}\")\n\n# Linear regression: gave ~ treatment\nmodel = smf.ols(\"gave ~ treatment\", data=df_k).fit()\nprint(model.summary())\n\nT-test result: t = 3.2095, df ≈ 36577, p = 0.0013\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Sat, 03 May 2025   Prob (F-statistic):            0.00193\nTime:                        12:44:07   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nTo test whether offering a matching donation influenced charitable behavior, I examined the response rate — the proportion of individuals who made any donation — across treatment and control groups.\n\n\nUsing a t-test, I found a statistically significant difference in donation rates:\n\nt = 3.21, degrees of freedom ≈ 36,577\n\np = 0.0013\n\nI also ran a bivariate linear regression:\n\ngave ~ treatment\n\nCoefficient on treatment: 0.0042,\n\nStandard error: 0.0014\n\np = 0.002\n\nThis confirms that individuals in the treatment group were significantly more likely to donate than those in the control group.\n\n\n\nThe estimated treatment effect is about 0.42 percentage points - small in absolute terms, but statistically meaningful. This suggests that framing a donation opportunity with a matching offer leads to an increased likelihood that someone contributes, even if the match amount or ratio isn’t overwhelming.\nIn other words, people are more responsive to the idea that their donation will be matched, likely because it enhances their perceived impact or makes the opportunity feel more urgent or meaningful.\n\n\n\nThese results align closely with the findings reported in Table 2A, Panel A of Karlan and List (2007):\n\nReported control group response rate: 1.8%\n\nReported treatment group response rate: 2.2%\n\nDifference: 0.004 (0.4 percentage points)\n\nThis nearly matches the 0.0042 effect size estimated in my regression. This agreement supports the accuracy of my replication and reinforces the study’s conclusion: matching offers boost donation behavior in a statistically and behaviorally significant way.\n\ndf_k['gave'] = (df_k['amount'] &gt; 0).astype(int)\n\n# Probit regression\nprobit_model = smf.probit(\"gave ~ treatment\", data=df_k).fit()\nprint(probit_model.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Sat, 03 May 2025   Pseudo R-squ.:               0.0009783\nTime:                        12:44:07   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n\n\n\n\n\n\nTo replicate Table 3 Column (1) from Karlan and List (2007), I ran a probit regression with the binary outcome variable gave (1 if a person donated, 0 otherwise), and the explanatory variable treatment (1 for treatment group, 0 for control).\nThe probit regression output was as follows:\n\nCoefficient on treatment: 0.087\nStandard error: 0.028\nz-value: 3.11\np-value: 0.002\n\nThis result is statistically significant at the 1% level, and the direction and significance match the findings reported in Table 3 of the original paper. While the numerical values differ slightly due to scale and model presentation, the key takeaway is consistent: treatment increases the likelihood of making a donation.\n\n\nThe probit model confirms that simply being told about a matching donation offer makes individuals more likely to give, even after accounting for the non-linear nature of the donation decision. This suggests that small behavioral cues, like highlighting a match, can meaningfully impact real-world choices. My findings support the original authors’ conclusion that match framing positively influences charitable behavior.\n\n\n\n\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\ndf_k['gave'] = (df_k['amount'] &gt; 0).astype(int)\n\ntreatments = df_k[df_k['treatment'] == 1]\n\n# Compare 1:1 vs 2:1\ngave_1 = treatments[treatments['ratio'] == 1]['gave']\ngave_2 = treatments[treatments['ratio'] == 2]['gave']\ntstat_12, pval_12 = stats.ttest_ind(gave_1, gave_2, equal_var=False)\n\n# Compare 1:1 vs 3:1\ngave_3 = treatments[treatments['ratio'] == 3]['gave']\ntstat_13, pval_13 = stats.ttest_ind(gave_1, gave_3, equal_var=False)\n\nprint(f\"1:1 vs 2:1 — t = {tstat_12:.4f}, p = {pval_12:.4f}\")\nprint(f\"1:1 vs 3:1 — t = {tstat_13:.4f}, p = {pval_13:.4f}\")\n\n1:1 vs 2:1 — t = -0.9650, p = 0.3345\n1:1 vs 3:1 — t = -1.0150, p = 0.3101\n\n\n\n\nTo evaluate whether larger match ratios (2:1 or 3:1) increase the likelihood of giving more than the standard 1:1 match, I conducted two separate t-tests on donation rates within the treatment group:\n\n1:1 vs 2:1 match ratio:\n\nt = –0.965, p = 0.335\n\n1:1 vs 3:1 match ratio:\n\nt = –1.015, p = 0.310\n\n\nIn both cases, the p-values are well above the 0.05 threshold, indicating no statistically significant difference in donation response rates between the 1:1 match and the higher match ratios.\nThese findings are consistent with the authors’ observation on page 8 of the paper:\n“Larger match ratios—$3:$1 and $2:$1—relative to smaller match ratios ($1:$1) have no additional impact.”\nMy results confirm that increasing the match ratio does not meaningfully change behavior. While one might expect that offering a 2:1 or 3:1 match would further incentivize donations, the evidence suggests that donors are primarily influenced by the presence of a match, rather than its size.\nThis aligns with the “figures suggest” comment in the paper, which points out that neither the size of the match ratio nor the match threshold meaningfully affected giving behavior, once the existence of a match was established.\nConclusion: A match offer matters, but bigger is not necessarily better. For fundraisers, this means that emphasizing the existence of a match may be more impactful than maximizing the match ratio.\n\ndf_ratios = df_k[(df_k['treatment'] == 1) & (df_k['ratio'].isin([1, 2, 3]))].copy()\ndf_ratios['gave'] = (df_ratios['amount'] &gt; 0).astype(int)\n\ndf_ratios['ratio'] = pd.Categorical(df_ratios['ratio'], categories=[1, 2, 3])\n\nmodel = smf.ols(\"gave ~ C(ratio)\", data=df_ratios).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.6454\nDate:                Sat, 03 May 2025   Prob (F-statistic):              0.524\nTime:                        12:44:08   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33393   BIC:                        -3.334e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         0.0207      0.001     14.912      0.000       0.018       0.023\nC(ratio)[T.2]     0.0019      0.002      0.958      0.338      -0.002       0.006\nC(ratio)[T.3]     0.0020      0.002      1.008      0.313      -0.002       0.006\n==============================================================================\nOmnibus:                    38963.957   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506478.937\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                         3.73\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\nTo assess whether larger matching ratios (2:1 or 3:1) lead to higher donation rates than a 1:1 match, I ran an OLS regression within the treatment group. The dependent variable gave indicates whether a donation was made, and the independent variable is the categorical match ratio (with 1:1 as the baseline).\n\n\n\nIntercept (1:1 match rate): 2.07% donation rate\n\n2:1 match rate: +0.19 percentage points (p = 0.338)\n\n3:1 match rate: +0.20 percentage points (p = 0.313)\n\nNone of the coefficients on the higher match ratios were statistically significant. The R-squared is essentially zero, meaning the model explains none of the variation in giving behavior based on match size alone.\n\n\n\nThese findings are consistent with earlier results and the original study by Karlan and List (2007). While one might expect larger match ratios to increase giving, the data show that donation likelihood does not significantly differ between 1:1, 2:1, and 3:1 matches.\nThe presence of a match matters more than its size. Increasing the match ratio does not significantly increase the probability that someone donates.\nThis reinforces the practical takeaway: simple match framing works, and there may be diminishing behavioral returns to making the match larger.\n\n# 1. Calculate directly from the data\ndf_ratios = df_k[(df_k['treatment'] == 1) & (df_k['ratio'].isin([1, 2, 3]))].copy()\ndf_ratios['gave'] = (df_ratios['amount'] &gt; 0).astype(int)\n\nresp_1 = df_ratios[df_ratios['ratio'] == 1]['gave'].mean()\nresp_2 = df_ratios[df_ratios['ratio'] == 2]['gave'].mean()\nresp_3 = df_ratios[df_ratios['ratio'] == 3]['gave'].mean()\n\n# Differences from raw data\ndiff_12_data = resp_2 - resp_1\ndiff_23_data = resp_3 - resp_2\n\n# 2. From fitted regression coefficients\ncoef_1 = 0.0207  \ncoef_2 = coef_1 + 0.0019 \ncoef_3 = coef_1 + 0.0020  \n\ndiff_12_model = coef_2 - coef_1\ndiff_23_model = coef_3 - coef_2\n\nprint(f\"Raw data differences:\")\nprint(f\"2:1 – 1:1 = {diff_12_data:.4f}\")\nprint(f\"3:1 – 2:1 = {diff_23_data:.4f}\\n\")\n\nprint(f\"Model-based differences (fitted values):\")\nprint(f\"2:1 – 1:1 = {diff_12_model:.4f}\")\nprint(f\"3:1 – 2:1 = {diff_23_model:.4f}\")\n\nRaw data differences:\n2:1 – 1:1 = 0.0019\n3:1 – 2:1 = 0.0001\n\nModel-based differences (fitted values):\n2:1 – 1:1 = 0.0019\n3:1 – 2:1 = 0.0001\n\n\n\n\n\n\nTo assess whether larger match ratios lead to higher donation rates, I compared the response rates between match levels using both raw data and fitted values from a regression model.\n\n\n\n2:1 – 1:1: 0.0019 (0.19 percentage points)\n3:1 – 2:1: 0.0001 (0.01 percentage points)\n\n\n\n\n\n2:1 – 1:1: 0.0019\n3:1 – 2:1: 0.0001\n\nThe results from both approaches are nearly identical, reinforcing their reliability. The increase in response rate from 1:1 to 2:1 is less than 0.2 percentage points, and the difference from 2:1 to 3:1 is effectively zero.\n\n\n\nThese findings provide strong evidence that increasing the match ratio beyond 1:1 does not meaningfully impact donation behavior. While the idea of a higher match may seem more generous or compelling, actual donor behavior appears largely indifferent to the size of the match.\nThe behavioral “nudge” comes from the presence of a match offer, not the magnitude of the match.\nThis aligns with the key insight of Karlan and List (2007): “larger match ratios… have no additional impact.”\n\n\n\n\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\ndf = df_k.copy()\n\n# 1. T-test on donation amount\namount_treat = df[df['treatment'] == 1]['amount']\namount_control = df[df['treatment'] == 0]['amount']\nt_stat, p_val = stats.ttest_ind(amount_treat, amount_control, equal_var=False)\n\n# 2. Regression of amount on treatment\nmodel = smf.ols(\"amount ~ treatment\", data=df).fit()\n\nprint(f\"T-test result: t = {t_stat:.4f}, p = {p_val:.4f}\")\nprint(model.summary())\n\nT-test result: t = 1.9183, p = 0.0551\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.461\nDate:                Sat, 03 May 2025   Prob (F-statistic):             0.0628\nTime:                        12:44:08   Log-Likelihood:            -1.7946e+05\nNo. Observations:               50083   AIC:                         3.589e+05\nDf Residuals:                   50081   BIC:                         3.589e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.8133      0.067     12.063      0.000       0.681       0.945\ntreatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n==============================================================================\nOmnibus:                    96861.113   Durbin-Watson:                   2.008\nProb(Omnibus):                  0.000   Jarque-Bera (JB):        240735713.635\nSkew:                          15.297   Prob(JB):                         0.00\nKurtosis:                     341.269   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nTo assess whether the matching donation offer affected not just the likelihood of giving, but also the size of the contribution, I conducted both a t-test and a linear regression of the donation amount on treatment status.\n\n\n\nt = 1.92\np = 0.055\n\nThis result is marginally statistically significant — just above the conventional 5% threshold. It suggests a potential difference in average donation amount between treatment and control groups.\n\n\n\n\nCoefficient on treatment: $0.15\np = 0.063\n\nThe regression indicates that individuals in the treatment group gave, on average, about 15 cents more than those in the control group. This difference is also marginally significant and should be interpreted cautiously.\nThese results suggest that the match offer may have a small effect on the size of donations, not just the likelihood of giving — but this effect is not robustly statistically significant. In practical terms, most of the impact of the match framing seems to come from getting more people to donate, rather than increasing how much they give.\nFor fundraisers, this distinction matters: matching appeals may increase participation, but not necessarily the donation amount per person.\n\ndf_positive = df_k[df_k['amount'] &gt; 0].copy()\n\nmodel_positive = smf.ols(\"amount ~ treatment\", data=df_positive).fit()\n\nprint(model_positive.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                    0.3374\nDate:                Sat, 03 May 2025   Prob (F-statistic):              0.561\nTime:                        12:44:08   Log-Likelihood:                -5326.8\nNo. Observations:                1034   AIC:                         1.066e+04\nDf Residuals:                    1032   BIC:                         1.067e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     45.5403      2.423     18.792      0.000      40.785      50.296\ntreatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\nOmnibus:                      587.258   Durbin-Watson:                   2.031\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5623.279\nSkew:                           2.464   Prob(JB):                         0.00\nKurtosis:                      13.307   Cond. No.                         3.49\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\nTo explore how the treatment affected donation size among those who gave, I limited the data to respondents with a positive donation amount and regressed amount on treatment.\n\n\n\nIntercept: $45.54\n\nTreatment effect: –$1.67\n\np-value: 0.561\n\nThis suggests that, among donors, those in the treatment group gave slightly less than those in the control group, by about $1.67 on average. However, this difference is not statistically significant, meaning we cannot rule out the possibility that the true difference is zero.\n\n\n\nThese results suggest that while the match offer may influence whether someone gives, it does not meaningfully affect the donation amount among those who already decided to give.\n\n\n\nIt’s important to note that this estimate does not have a causal interpretation. While treatment assignment was randomized, we are now analyzing a non-random subset: people who chose to give.\nBecause giving is influenced by the treatment, conditioning on it introduces selection bias, the comparison may now be confounded by unobserved differences between donors in the treatment and control groups.\nIn short: we can’t say the treatment caused donors to give less — we’re just observing that among givers, the average gift was slightly lower in the treatment group.\nThe key takeaway remains: matching increases participation, but not necessarily contribution\n\ndonors = df_k[df_k['amount'] &gt; 0].copy()\n\ndonors_treat = donors[donors['treatment'] == 1]\ndonors_control = donors[donors['treatment'] == 0]\n\nmean_treat = donors_treat['amount'].mean()\nmean_control = donors_control['amount'].mean()\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\naxes[0].hist(donors_treat['amount'], bins=30, color='skyblue', edgecolor='black')\naxes[0].axvline(mean_treat, color='red', linestyle='--', label=f'Mean = ${mean_treat:.2f}')\naxes[0].set_title('Treatment Group')\naxes[0].set_xlabel('Donation Amount')\naxes[0].set_ylabel('Number of Donors')\naxes[0].legend()\n\naxes[1].hist(donors_control['amount'], bins=30, color='lightgreen', edgecolor='black')\naxes[1].axvline(mean_control, color='red', linestyle='--', label=f'Mean = ${mean_control:.2f}')\naxes[1].set_title('Control Group')\naxes[1].set_xlabel('Donation Amount')\naxes[1].legend()\n\nplt.suptitle('Distribution of Donation Amounts (Among Donors)', fontsize=14)\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\n\n\nnp.random.seed(42)\n\n# Control group: Bernoulli(p=0.018), 100,000 draws\ncontrol_sim = np.random.binomial(n=1, p=0.018, size=100000)\n\n# Treatment group: Bernoulli(p=0.022), 10,000 draws\ntreatment_sim = np.random.binomial(n=1, p=0.022, size=10000)\n\n# diff_vector = treatment_sim - np.random.choice(control_sim, size=10000)\ndiff_vector = treatment_sim - control_sim[:10000]\n\ncumulative_avg = np.cumsum(diff_vector) / np.arange(1, len(diff_vector) + 1)\n\ntrue_diff = 0.022 - 0.018\n\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label='Cumulative Average of Differences')\nplt.axhline(y=true_diff, color='red', linestyle='--', label=f'True Difference = {true_diff:.3f}')\nplt.title('Law of Large Numbers: Cumulative Avg of Bernoulli Differences (Treatment - Control)')\nplt.xlabel('Number of Simulated Samples')\nplt.ylabel('Cumulative Average Difference')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe plot above shows the cumulative average difference between simulated donations in the treatment and control groups over 10,000 trials. Each trial compares a Bernoulli draw from the treatment distribution (p = 0.022) to one from the control distribution (p = 0.018).\nAs expected under the Law of Large Numbers, the cumulative average initially fluctuates substantially due to randomness in small samples. However, as the number of simulations increases, the average steadily stabilizes and converges toward the true difference in means: 0.004.\nBy around 5,000 simulations, the cumulative average remains tightly clustered around the true value, with only minor variation. This illustrates that: With enough independent observations, the sample average of a statistic becomes a reliable estimate of its population value.\n\n\n\n\nsample_sizes = [50, 200, 500, 1000]\nsimulations = 1000\np_control = 0.018\np_treatment = 0.022\n\ndiff_distributions = {}\n\nnp.random.seed(42)\n\nfor n in sample_sizes:\n    diffs = []\n    for _ in range(simulations):\n        control_draw = np.random.binomial(1, p_control, n)\n        treatment_draw = np.random.binomial(1, p_treatment, n)\n        mean_diff = treatment_draw.mean() - control_draw.mean()\n        diffs.append(mean_diff)\n    diff_distributions[n] = diffs\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    axes[i].hist(diff_distributions[n], bins=30, edgecolor='black', alpha=0.7)\n    axes[i].axvline(0, color='black', linestyle='--', label='Zero')\n    axes[i].set_title(f\"Sample Size = {n}\")\n    axes[i].set_xlabel(\"Mean Difference (Treatment - Control)\")\n    axes[i].axvline(p_treatment - p_control, color='red', linestyle='--', label='True Difference')\n    axes[i].set_ylabel(\"Frequency\")\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe plots above show the sampling distributions of the difference in means between the treatment and control groups, based on repeated simulations for different sample sizes: n = 50, 200, 500, and 1000.\nEach distribution includes: - A red dashed line for the true population difference (0.004) - A black dashed line at zero (the null hypothesis of no difference)\n\n\n\nAt small sample sizes (n = 50, 200), the distributions are wide and noisy, and the value zero lies well within the main body of the distribution. This means it would be difficult to reject the null hypothesis — the sampling variability is too high.\nAt larger sample sizes (n = 500, 1000), the distribution becomes narrower and more bell-shaped, and the value of zero begins to move toward the tail of the distribution. This indicates that larger samples provide more precise estimates, and it becomes easier to detect small differences like the true 0.004 effect.\n\n\n\n\nAs the sample size increases, the sampling distribution becomes tighter and better centered around the true value — a classic demonstration of the Central Limit Theorem.\nAt larger sample sizes, zero falls closer to the tail, suggesting that with sufficient data, we would be more likely to detect a statistically significant treatment effect (even when that effect is small).\nThis reinforces the importance of sample size in hypothesis testing: small effects can only be detected when the noise is small — which is what large n gives us."
  },
  {
    "objectID": "blog/project1/index.html#introduction",
    "href": "blog/project1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis large-scale natural field experiment was designed to explore whether price incentives, framed through matching gifts, influence charitable giving behavior. In the treatment group, donors were told that a “concerned fellow member” would match their donation at varying rates: 1:1, 2:1, or 3:1. Each donor was randomly assigned to one of these match ratios, to a specific suggested donation amount, and to a maximum gift match cap. In contrast, the control group received a standard appeal letter without any mention of a match.\nThe study investigates how these price frames affect both the likelihood of giving and the amount donated. Importantly, the match offers were real and implemented through a conditional agreement with anonymous donors. The paper contributes to the economic literature by testing whether traditional assumptions about donor price sensitivity hold in real-world charitable giving, an area long theorized but rarely tested at this scale and with this level of experimental control.\nThis report replicates and extends aspects of Karlan and List’s analysis using their public dataset. Specifically, I analyze whether matched donations lead to higher response rates, whether larger match ratios produce stronger effects, and whether the treatment influenced the size of contributions. I also use simulation to illustrate key statistical concepts such as the Law of Large Numbers and the Central Limit Theorem, which underpin the validity of the experimental estimates.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/index.html#data",
    "href": "blog/project1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\ndf_k = pd.read_stata(\"/home/jovyan/Desktop/Marketing Analytics Website : HW's/karlan_list_2007.dta\")\ndf_k\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.000000\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.000000\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.000000\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n50078\n1\n0\n1\n0\n0\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.872797\n0.089959\n0.257265\n2.13\n45047.0\n0.771316\n0.263744\n1.000000\n\n\n50079\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.688262\n0.108889\n0.288792\n2.67\n74655.0\n0.741931\n0.586466\n1.000000\n\n\n50080\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\n0.900000\n0.021311\n0.178689\n2.36\n26667.0\n0.778689\n0.107930\n0.000000\n\n\n50081\n1\n0\n3\n0\n1\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.917206\n0.008257\n0.225619\n2.57\n39530.0\n0.733988\n0.184768\n0.634903\n\n\n50082\n1\n0\n3\n0\n1\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.530023\n0.074112\n0.340698\n3.70\n48744.0\n0.717843\n0.127941\n0.994181\n\n\n\n\n50083 rows × 51 columns\n\n\n\nThe dataset consists of 50,083 observations and 51 variables, and appears to originate from a field experiment or observational study, likely related to donation behavior or outreach strategies. Each row represents an individual unit (e.g., a household or donor), and the dataset includes both treatment assignments and a range of covariates.\n\n\n\ntreatment, control: Binary indicators denoting experimental group assignment. A value of 1 in treatment indicates the subject received the treatment condition, while 1 in control indicates assignment to the control group. These two variables are mutually exclusive.\nratio, ratio2, ratio3: Variables reflecting matching incentives (e.g., 1:1, 2:1 matches) provided during the intervention. These are likely categorical or indicator variables showing the ratio level applied.\nsize, size25, size50, size100, sizeno: Donation ask size variables. The size column includes textual representations of the suggested donation amount (e.g., \"$100,000\"), while the other columns serve as binary indicators for specific size categories.\n\n\n\n\n\nredcty, bluecty: Indicators of political affiliation or voting patterns in the subject’s county or city.\npwhite, pblack: Proportion of white and Black residents in the geographic area.\npage18_39: Proportion of the population between ages 18 and 39.\nave_hh_sz: Average household size.\nmedian_hhincome: Median household income in the area.\npowner: Proportion of homeowners.\npsch_atlstba: Proportion of the population with at least a bachelor’s degree.\npop_propurban: Proportion of the population living in urban areas.\n\n\n\n\nSeveral rows contain missing values (NaN), especially in demographic variables like pwhite, pblack, and median_hhincome. These may result from incomplete geographic data or limitations in census coverage.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\n\n\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nTo assess whether the random assignment mechanism was successful, I conduct balance tests on three baseline variables that are not influenced by treatment:\n\nmrm2: months since last donation\n\npwhite: proportion of white residents in the donor’s area\n\nave_hh_sz: average household size\n\nFor each variable, I apply both a manual t-test and a linear regression to compare treatment and control groups. These methods should yield consistent results, since both estimate the same underlying difference in means.\n\nimport statsmodels.formula.api as smf\nfrom scipy import stats\n\n# Function to compute manual t-test and regression for a given variable\ndef balance_test(df, variable, treatment_col=\"treatment\"):\n    df_clean = df.dropna(subset=[variable, treatment_col])\n    treat = df_clean[df_clean[treatment_col] == 1][variable]\n    control = df_clean[df_clean[treatment_col] == 0][variable]\n\n    # Manual t-test\n    mean_treat = treat.mean()\n    mean_control = control.mean()\n    var_treat = treat.var(ddof=1)\n    var_control = control.var(ddof=1)\n    n_treat = len(treat)\n    n_control = len(control)\n    se = np.sqrt(var_treat/n_treat + var_control/n_control)\n    t_stat = (mean_treat - mean_control) / se\n\n    # Welch's degrees of freedom\n    df_num = (var_treat/n_treat + var_control/n_control)**2\n    df_denom = ((var_treat/n_treat)**2 / (n_treat - 1)) + ((var_control/n_control)**2 / (n_control - 1))\n    df_eff = df_num / df_denom\n\n    print(f\"\\n=== Balance Test: {variable} ===\")\n    print(f\"Manual t-test: t = {t_stat:.4f}, df ≈ {df_eff:.2f}\")\n\n    # Linear regression\n    formula = f\"{variable} ~ {treatment_col}\"\n    model = smf.ols(formula, data=df_clean).fit()\n    coef = model.params[treatment_col]\n    pval = model.pvalues[treatment_col]\n    print(f\"Regression coefficient on treatment: {coef:.4f}, p = {pval:.4f}\")\n    return model.summary()\n\nbalance_test(df_k, \"mrm2\")\nbalance_test(df_k, \"pwhite\")\nbalance_test(df_k, \"ave_hh_sz\")\n\n\n=== Balance Test: mrm2 ===\nManual t-test: t = 0.1195, df ≈ 33394.48\nRegression coefficient on treatment: 0.0137, p = 0.9049\n\n=== Balance Test: pwhite ===\nManual t-test: t = -0.5590, df ≈ 31876.22\nRegression coefficient on treatment: -0.0009, p = 0.5753\n\n=== Balance Test: ave_hh_sz ===\nManual t-test: t = 0.8234, df ≈ 31960.02\nRegression coefficient on treatment: 0.0030, p = 0.4098\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nave_hh_sz\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.6794\n\n\nDate:\nSat, 03 May 2025\nProb (F-statistic):\n0.410\n\n\nTime:\n12:44:07\nLog-Likelihood:\n-21524.\n\n\nNo. Observations:\n48221\nAIC:\n4.305e+04\n\n\nDf Residuals:\n48219\nBIC:\n4.307e+04\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n2.4270\n0.003\n812.995\n0.000\n2.421\n2.433\n\n\ntreatment\n0.0030\n0.004\n0.824\n0.410\n-0.004\n0.010\n\n\n\n\n\n\n\n\nOmnibus:\n1488.463\nDurbin-Watson:\n2.003\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n4066.993\n\n\nSkew:\n0.007\nProb(JB):\n0.00\n\n\nKurtosis:\n4.423\nCond. No.\n3.23\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nManual t-test:\n\nt-statistic: 0.1195, degrees of freedom ≈ 33,394, p = 0.905\nNot statistically significant\n\nRegression (mrm2 ~ treatment):\n\nCoefficient on treatment: 0.0137\np-value: 0.905\n\n\nInterpretation: There is no statistically significant difference in prior giving recency between the groups, indicating balance.\n\n\n\n\n\nManual t-test:\n\nt-statistic: –0.5590, degrees of freedom ≈ 31,876, p = 0.5753\nNot statistically significant\n\nRegression (pwhite ~ treatment):\n\nCoefficient on treatment: –0.0009\np-value: 0.5753\n\n\nInterpretation: Racial composition across treatment and control groups is nearly identical, with no meaningful difference detected.\n\n\n\n\n\nManual t-test:\n\nt-statistic: 0.8234, degrees of freedom ≈ 31,960, p = 0.4108\nNot statistically significant\n\nRegression (ave_hh_sz ~ treatment):\n\nCoefficient on treatment: 0.0030\np-value: 0.4108\n\n\nInterpretation: Household size is well balanced between the groups, with the regression confirming no significant difference.\n\n\n\n\nThe results from all three variables show no statistically significant differences between the treatment and control groups. This supports the integrity of the randomization process and aligns with Table 1 in Karlan and List (2007), which was included in the original paper for the same purpose.\nBy demonstrating that key baseline characteristics are similar across groups, we can be more confident that any observed differences in outcome variables can be attributed to the treatment itself rather than to underlying differences in the populations.\nThe balance tests confirm that random assignment was successfully implemented in the experiment."
  },
  {
    "objectID": "blog/project1/index.html#experimental-results",
    "href": "blog/project1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "First, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nimport matplotlib.pyplot as plt\n\ndf_k['gave'] = (df_k['amount'] &gt; 0).astype(int)\n\nresponse_rates = df_k.groupby('treatment')['gave'].mean()\nlabels = ['Control', 'Treatment']\n\nplt.figure(figsize=(6, 4))\nplt.bar(labels, response_rates, color=['gray', 'blue'])\nplt.ylabel('Proportion Donated')\nplt.title('Response Rate by Group')\nplt.ylim(0, 0.05) \nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ndf_k['gave'] = (df_k['amount'] &gt; 0).astype(int)\n\ngave_treat = df_k[df_k['treatment'] == 1]['gave']\ngave_control = df_k[df_k['treatment'] == 0]['gave']\n\n# Manual t-test (Welch's t-test)\nmean_treat = gave_treat.mean()\nmean_control = gave_control.mean()\nvar_treat = gave_treat.var(ddof=1)\nvar_control = gave_control.var(ddof=1)\nn_treat = len(gave_treat)\nn_control = len(gave_control)\n\nse = np.sqrt(var_treat/n_treat + var_control/n_control)\nt_stat = (mean_treat - mean_control) / se\n\ndf_num = (var_treat/n_treat + var_control/n_control)**2\ndf_denom = ((var_treat/n_treat)**2 / (n_treat - 1)) + ((var_control/n_control)**2 / (n_control - 1))\ndf_eff = df_num / df_denom\n\np_val = 2 * (1 - stats.t.cdf(abs(t_stat), df=df_eff))\n\nprint(f\"T-test result: t = {t_stat:.4f}, df ≈ {df_eff:.0f}, p = {p_val:.4f}\")\n\n# Linear regression: gave ~ treatment\nmodel = smf.ols(\"gave ~ treatment\", data=df_k).fit()\nprint(model.summary())\n\nT-test result: t = 3.2095, df ≈ 36577, p = 0.0013\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Sat, 03 May 2025   Prob (F-statistic):            0.00193\nTime:                        12:44:07   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nTo test whether offering a matching donation influenced charitable behavior, I examined the response rate — the proportion of individuals who made any donation — across treatment and control groups.\n\n\nUsing a t-test, I found a statistically significant difference in donation rates:\n\nt = 3.21, degrees of freedom ≈ 36,577\n\np = 0.0013\n\nI also ran a bivariate linear regression:\n\ngave ~ treatment\n\nCoefficient on treatment: 0.0042,\n\nStandard error: 0.0014\n\np = 0.002\n\nThis confirms that individuals in the treatment group were significantly more likely to donate than those in the control group.\n\n\n\nThe estimated treatment effect is about 0.42 percentage points - small in absolute terms, but statistically meaningful. This suggests that framing a donation opportunity with a matching offer leads to an increased likelihood that someone contributes, even if the match amount or ratio isn’t overwhelming.\nIn other words, people are more responsive to the idea that their donation will be matched, likely because it enhances their perceived impact or makes the opportunity feel more urgent or meaningful.\n\n\n\nThese results align closely with the findings reported in Table 2A, Panel A of Karlan and List (2007):\n\nReported control group response rate: 1.8%\n\nReported treatment group response rate: 2.2%\n\nDifference: 0.004 (0.4 percentage points)\n\nThis nearly matches the 0.0042 effect size estimated in my regression. This agreement supports the accuracy of my replication and reinforces the study’s conclusion: matching offers boost donation behavior in a statistically and behaviorally significant way.\n\ndf_k['gave'] = (df_k['amount'] &gt; 0).astype(int)\n\n# Probit regression\nprobit_model = smf.probit(\"gave ~ treatment\", data=df_k).fit()\nprint(probit_model.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Sat, 03 May 2025   Pseudo R-squ.:               0.0009783\nTime:                        12:44:07   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n\n\n\n\n\n\nTo replicate Table 3 Column (1) from Karlan and List (2007), I ran a probit regression with the binary outcome variable gave (1 if a person donated, 0 otherwise), and the explanatory variable treatment (1 for treatment group, 0 for control).\nThe probit regression output was as follows:\n\nCoefficient on treatment: 0.087\nStandard error: 0.028\nz-value: 3.11\np-value: 0.002\n\nThis result is statistically significant at the 1% level, and the direction and significance match the findings reported in Table 3 of the original paper. While the numerical values differ slightly due to scale and model presentation, the key takeaway is consistent: treatment increases the likelihood of making a donation.\n\n\nThe probit model confirms that simply being told about a matching donation offer makes individuals more likely to give, even after accounting for the non-linear nature of the donation decision. This suggests that small behavioral cues, like highlighting a match, can meaningfully impact real-world choices. My findings support the original authors’ conclusion that match framing positively influences charitable behavior.\n\n\n\n\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\ndf_k['gave'] = (df_k['amount'] &gt; 0).astype(int)\n\ntreatments = df_k[df_k['treatment'] == 1]\n\n# Compare 1:1 vs 2:1\ngave_1 = treatments[treatments['ratio'] == 1]['gave']\ngave_2 = treatments[treatments['ratio'] == 2]['gave']\ntstat_12, pval_12 = stats.ttest_ind(gave_1, gave_2, equal_var=False)\n\n# Compare 1:1 vs 3:1\ngave_3 = treatments[treatments['ratio'] == 3]['gave']\ntstat_13, pval_13 = stats.ttest_ind(gave_1, gave_3, equal_var=False)\n\nprint(f\"1:1 vs 2:1 — t = {tstat_12:.4f}, p = {pval_12:.4f}\")\nprint(f\"1:1 vs 3:1 — t = {tstat_13:.4f}, p = {pval_13:.4f}\")\n\n1:1 vs 2:1 — t = -0.9650, p = 0.3345\n1:1 vs 3:1 — t = -1.0150, p = 0.3101\n\n\n\n\nTo evaluate whether larger match ratios (2:1 or 3:1) increase the likelihood of giving more than the standard 1:1 match, I conducted two separate t-tests on donation rates within the treatment group:\n\n1:1 vs 2:1 match ratio:\n\nt = –0.965, p = 0.335\n\n1:1 vs 3:1 match ratio:\n\nt = –1.015, p = 0.310\n\n\nIn both cases, the p-values are well above the 0.05 threshold, indicating no statistically significant difference in donation response rates between the 1:1 match and the higher match ratios.\nThese findings are consistent with the authors’ observation on page 8 of the paper:\n“Larger match ratios—$3:$1 and $2:$1—relative to smaller match ratios ($1:$1) have no additional impact.”\nMy results confirm that increasing the match ratio does not meaningfully change behavior. While one might expect that offering a 2:1 or 3:1 match would further incentivize donations, the evidence suggests that donors are primarily influenced by the presence of a match, rather than its size.\nThis aligns with the “figures suggest” comment in the paper, which points out that neither the size of the match ratio nor the match threshold meaningfully affected giving behavior, once the existence of a match was established.\nConclusion: A match offer matters, but bigger is not necessarily better. For fundraisers, this means that emphasizing the existence of a match may be more impactful than maximizing the match ratio.\n\ndf_ratios = df_k[(df_k['treatment'] == 1) & (df_k['ratio'].isin([1, 2, 3]))].copy()\ndf_ratios['gave'] = (df_ratios['amount'] &gt; 0).astype(int)\n\ndf_ratios['ratio'] = pd.Categorical(df_ratios['ratio'], categories=[1, 2, 3])\n\nmodel = smf.ols(\"gave ~ C(ratio)\", data=df_ratios).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.6454\nDate:                Sat, 03 May 2025   Prob (F-statistic):              0.524\nTime:                        12:44:08   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33393   BIC:                        -3.334e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         0.0207      0.001     14.912      0.000       0.018       0.023\nC(ratio)[T.2]     0.0019      0.002      0.958      0.338      -0.002       0.006\nC(ratio)[T.3]     0.0020      0.002      1.008      0.313      -0.002       0.006\n==============================================================================\nOmnibus:                    38963.957   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506478.937\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                         3.73\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\nTo assess whether larger matching ratios (2:1 or 3:1) lead to higher donation rates than a 1:1 match, I ran an OLS regression within the treatment group. The dependent variable gave indicates whether a donation was made, and the independent variable is the categorical match ratio (with 1:1 as the baseline).\n\n\n\nIntercept (1:1 match rate): 2.07% donation rate\n\n2:1 match rate: +0.19 percentage points (p = 0.338)\n\n3:1 match rate: +0.20 percentage points (p = 0.313)\n\nNone of the coefficients on the higher match ratios were statistically significant. The R-squared is essentially zero, meaning the model explains none of the variation in giving behavior based on match size alone.\n\n\n\nThese findings are consistent with earlier results and the original study by Karlan and List (2007). While one might expect larger match ratios to increase giving, the data show that donation likelihood does not significantly differ between 1:1, 2:1, and 3:1 matches.\nThe presence of a match matters more than its size. Increasing the match ratio does not significantly increase the probability that someone donates.\nThis reinforces the practical takeaway: simple match framing works, and there may be diminishing behavioral returns to making the match larger.\n\n# 1. Calculate directly from the data\ndf_ratios = df_k[(df_k['treatment'] == 1) & (df_k['ratio'].isin([1, 2, 3]))].copy()\ndf_ratios['gave'] = (df_ratios['amount'] &gt; 0).astype(int)\n\nresp_1 = df_ratios[df_ratios['ratio'] == 1]['gave'].mean()\nresp_2 = df_ratios[df_ratios['ratio'] == 2]['gave'].mean()\nresp_3 = df_ratios[df_ratios['ratio'] == 3]['gave'].mean()\n\n# Differences from raw data\ndiff_12_data = resp_2 - resp_1\ndiff_23_data = resp_3 - resp_2\n\n# 2. From fitted regression coefficients\ncoef_1 = 0.0207  \ncoef_2 = coef_1 + 0.0019 \ncoef_3 = coef_1 + 0.0020  \n\ndiff_12_model = coef_2 - coef_1\ndiff_23_model = coef_3 - coef_2\n\nprint(f\"Raw data differences:\")\nprint(f\"2:1 – 1:1 = {diff_12_data:.4f}\")\nprint(f\"3:1 – 2:1 = {diff_23_data:.4f}\\n\")\n\nprint(f\"Model-based differences (fitted values):\")\nprint(f\"2:1 – 1:1 = {diff_12_model:.4f}\")\nprint(f\"3:1 – 2:1 = {diff_23_model:.4f}\")\n\nRaw data differences:\n2:1 – 1:1 = 0.0019\n3:1 – 2:1 = 0.0001\n\nModel-based differences (fitted values):\n2:1 – 1:1 = 0.0019\n3:1 – 2:1 = 0.0001\n\n\n\n\n\n\nTo assess whether larger match ratios lead to higher donation rates, I compared the response rates between match levels using both raw data and fitted values from a regression model.\n\n\n\n2:1 – 1:1: 0.0019 (0.19 percentage points)\n3:1 – 2:1: 0.0001 (0.01 percentage points)\n\n\n\n\n\n2:1 – 1:1: 0.0019\n3:1 – 2:1: 0.0001\n\nThe results from both approaches are nearly identical, reinforcing their reliability. The increase in response rate from 1:1 to 2:1 is less than 0.2 percentage points, and the difference from 2:1 to 3:1 is effectively zero.\n\n\n\nThese findings provide strong evidence that increasing the match ratio beyond 1:1 does not meaningfully impact donation behavior. While the idea of a higher match may seem more generous or compelling, actual donor behavior appears largely indifferent to the size of the match.\nThe behavioral “nudge” comes from the presence of a match offer, not the magnitude of the match.\nThis aligns with the key insight of Karlan and List (2007): “larger match ratios… have no additional impact.”\n\n\n\n\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\ndf = df_k.copy()\n\n# 1. T-test on donation amount\namount_treat = df[df['treatment'] == 1]['amount']\namount_control = df[df['treatment'] == 0]['amount']\nt_stat, p_val = stats.ttest_ind(amount_treat, amount_control, equal_var=False)\n\n# 2. Regression of amount on treatment\nmodel = smf.ols(\"amount ~ treatment\", data=df).fit()\n\nprint(f\"T-test result: t = {t_stat:.4f}, p = {p_val:.4f}\")\nprint(model.summary())\n\nT-test result: t = 1.9183, p = 0.0551\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.461\nDate:                Sat, 03 May 2025   Prob (F-statistic):             0.0628\nTime:                        12:44:08   Log-Likelihood:            -1.7946e+05\nNo. Observations:               50083   AIC:                         3.589e+05\nDf Residuals:                   50081   BIC:                         3.589e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.8133      0.067     12.063      0.000       0.681       0.945\ntreatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n==============================================================================\nOmnibus:                    96861.113   Durbin-Watson:                   2.008\nProb(Omnibus):                  0.000   Jarque-Bera (JB):        240735713.635\nSkew:                          15.297   Prob(JB):                         0.00\nKurtosis:                     341.269   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nTo assess whether the matching donation offer affected not just the likelihood of giving, but also the size of the contribution, I conducted both a t-test and a linear regression of the donation amount on treatment status.\n\n\n\nt = 1.92\np = 0.055\n\nThis result is marginally statistically significant — just above the conventional 5% threshold. It suggests a potential difference in average donation amount between treatment and control groups.\n\n\n\n\nCoefficient on treatment: $0.15\np = 0.063\n\nThe regression indicates that individuals in the treatment group gave, on average, about 15 cents more than those in the control group. This difference is also marginally significant and should be interpreted cautiously.\nThese results suggest that the match offer may have a small effect on the size of donations, not just the likelihood of giving — but this effect is not robustly statistically significant. In practical terms, most of the impact of the match framing seems to come from getting more people to donate, rather than increasing how much they give.\nFor fundraisers, this distinction matters: matching appeals may increase participation, but not necessarily the donation amount per person.\n\ndf_positive = df_k[df_k['amount'] &gt; 0].copy()\n\nmodel_positive = smf.ols(\"amount ~ treatment\", data=df_positive).fit()\n\nprint(model_positive.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                    0.3374\nDate:                Sat, 03 May 2025   Prob (F-statistic):              0.561\nTime:                        12:44:08   Log-Likelihood:                -5326.8\nNo. Observations:                1034   AIC:                         1.066e+04\nDf Residuals:                    1032   BIC:                         1.067e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     45.5403      2.423     18.792      0.000      40.785      50.296\ntreatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\nOmnibus:                      587.258   Durbin-Watson:                   2.031\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5623.279\nSkew:                           2.464   Prob(JB):                         0.00\nKurtosis:                      13.307   Cond. No.                         3.49\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\nTo explore how the treatment affected donation size among those who gave, I limited the data to respondents with a positive donation amount and regressed amount on treatment.\n\n\n\nIntercept: $45.54\n\nTreatment effect: –$1.67\n\np-value: 0.561\n\nThis suggests that, among donors, those in the treatment group gave slightly less than those in the control group, by about $1.67 on average. However, this difference is not statistically significant, meaning we cannot rule out the possibility that the true difference is zero.\n\n\n\nThese results suggest that while the match offer may influence whether someone gives, it does not meaningfully affect the donation amount among those who already decided to give.\n\n\n\nIt’s important to note that this estimate does not have a causal interpretation. While treatment assignment was randomized, we are now analyzing a non-random subset: people who chose to give.\nBecause giving is influenced by the treatment, conditioning on it introduces selection bias, the comparison may now be confounded by unobserved differences between donors in the treatment and control groups.\nIn short: we can’t say the treatment caused donors to give less — we’re just observing that among givers, the average gift was slightly lower in the treatment group.\nThe key takeaway remains: matching increases participation, but not necessarily contribution\n\ndonors = df_k[df_k['amount'] &gt; 0].copy()\n\ndonors_treat = donors[donors['treatment'] == 1]\ndonors_control = donors[donors['treatment'] == 0]\n\nmean_treat = donors_treat['amount'].mean()\nmean_control = donors_control['amount'].mean()\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\naxes[0].hist(donors_treat['amount'], bins=30, color='skyblue', edgecolor='black')\naxes[0].axvline(mean_treat, color='red', linestyle='--', label=f'Mean = ${mean_treat:.2f}')\naxes[0].set_title('Treatment Group')\naxes[0].set_xlabel('Donation Amount')\naxes[0].set_ylabel('Number of Donors')\naxes[0].legend()\n\naxes[1].hist(donors_control['amount'], bins=30, color='lightgreen', edgecolor='black')\naxes[1].axvline(mean_control, color='red', linestyle='--', label=f'Mean = ${mean_control:.2f}')\naxes[1].set_title('Control Group')\naxes[1].set_xlabel('Donation Amount')\naxes[1].legend()\n\nplt.suptitle('Distribution of Donation Amounts (Among Donors)', fontsize=14)\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()"
  },
  {
    "objectID": "blog/project1/index.html#simulation-experiment",
    "href": "blog/project1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "As a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\n\n\nnp.random.seed(42)\n\n# Control group: Bernoulli(p=0.018), 100,000 draws\ncontrol_sim = np.random.binomial(n=1, p=0.018, size=100000)\n\n# Treatment group: Bernoulli(p=0.022), 10,000 draws\ntreatment_sim = np.random.binomial(n=1, p=0.022, size=10000)\n\n# diff_vector = treatment_sim - np.random.choice(control_sim, size=10000)\ndiff_vector = treatment_sim - control_sim[:10000]\n\ncumulative_avg = np.cumsum(diff_vector) / np.arange(1, len(diff_vector) + 1)\n\ntrue_diff = 0.022 - 0.018\n\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label='Cumulative Average of Differences')\nplt.axhline(y=true_diff, color='red', linestyle='--', label=f'True Difference = {true_diff:.3f}')\nplt.title('Law of Large Numbers: Cumulative Avg of Bernoulli Differences (Treatment - Control)')\nplt.xlabel('Number of Simulated Samples')\nplt.ylabel('Cumulative Average Difference')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe plot above shows the cumulative average difference between simulated donations in the treatment and control groups over 10,000 trials. Each trial compares a Bernoulli draw from the treatment distribution (p = 0.022) to one from the control distribution (p = 0.018).\nAs expected under the Law of Large Numbers, the cumulative average initially fluctuates substantially due to randomness in small samples. However, as the number of simulations increases, the average steadily stabilizes and converges toward the true difference in means: 0.004.\nBy around 5,000 simulations, the cumulative average remains tightly clustered around the true value, with only minor variation. This illustrates that: With enough independent observations, the sample average of a statistic becomes a reliable estimate of its population value.\n\n\n\n\nsample_sizes = [50, 200, 500, 1000]\nsimulations = 1000\np_control = 0.018\np_treatment = 0.022\n\ndiff_distributions = {}\n\nnp.random.seed(42)\n\nfor n in sample_sizes:\n    diffs = []\n    for _ in range(simulations):\n        control_draw = np.random.binomial(1, p_control, n)\n        treatment_draw = np.random.binomial(1, p_treatment, n)\n        mean_diff = treatment_draw.mean() - control_draw.mean()\n        diffs.append(mean_diff)\n    diff_distributions[n] = diffs\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    axes[i].hist(diff_distributions[n], bins=30, edgecolor='black', alpha=0.7)\n    axes[i].axvline(0, color='black', linestyle='--', label='Zero')\n    axes[i].set_title(f\"Sample Size = {n}\")\n    axes[i].set_xlabel(\"Mean Difference (Treatment - Control)\")\n    axes[i].axvline(p_treatment - p_control, color='red', linestyle='--', label='True Difference')\n    axes[i].set_ylabel(\"Frequency\")\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe plots above show the sampling distributions of the difference in means between the treatment and control groups, based on repeated simulations for different sample sizes: n = 50, 200, 500, and 1000.\nEach distribution includes: - A red dashed line for the true population difference (0.004) - A black dashed line at zero (the null hypothesis of no difference)\n\n\n\nAt small sample sizes (n = 50, 200), the distributions are wide and noisy, and the value zero lies well within the main body of the distribution. This means it would be difficult to reject the null hypothesis — the sampling variability is too high.\nAt larger sample sizes (n = 500, 1000), the distribution becomes narrower and more bell-shaped, and the value of zero begins to move toward the tail of the distribution. This indicates that larger samples provide more precise estimates, and it becomes easier to detect small differences like the true 0.004 effect.\n\n\n\n\nAs the sample size increases, the sampling distribution becomes tighter and better centered around the true value — a classic demonstration of the Central Limit Theorem.\nAt larger sample sizes, zero falls closer to the tail, suggesting that with sufficient data, we would be more likely to detect a statistically significant treatment effect (even when that effect is small).\nThis reinforces the importance of sample size in hypothesis testing: small effects can only be detected when the noise is small — which is what large n gives us."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alexa Gamble",
    "section": "",
    "text": "Here is a paragraph about me!"
  },
  {
    "objectID": "blog/project2/index.html",
    "href": "blog/project2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nblueprinty = pd.read_csv(\"/home/jovyan/Desktop/Marketing Analytics Website : HW's/blueprinty.csv\")\nairbnb = pd.read_csv(\"/home/jovyan/Desktop/Marketing Analytics Website : HW's/airbnb.csv\")\n\nblueprinty\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n...\n...\n...\n...\n...\n\n\n1495\n2\nNortheast\n18.5\n1\n\n\n1496\n3\nSouthwest\n22.5\n0\n\n\n1497\n4\nSouthwest\n17.0\n0\n\n\n1498\n3\nSouth\n29.0\n0\n\n\n1499\n1\nSouth\n39.0\n0\n\n\n\n\n1500 rows × 4 columns\n\n\n\n\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\ncustomers = blueprinty[blueprinty['iscustomer'] == 1]\nnon_customers = blueprinty[blueprinty['iscustomer'] == 0]\n\nplt.figure(figsize=(10, 5))\nplt.hist(customers['patents'], bins=20, alpha=0.6, label='Customers', edgecolor='black')\nplt.hist(non_customers['patents'], bins=20, alpha=0.6, label='Non-Customers', edgecolor='black')\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histogram of Number of Patents by Customer Status')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nmean_customers = customers['patents'].mean()\nmean_non_customers = non_customers['patents'].mean()\n\nprint(f'Customer Mean Number of Patents: {mean_customers}')\nprint(f'Non-Customer Mean Number of Patents: {mean_non_customers}')\n\n\n\n\n\n\n\n\nCustomer Mean Number of Patents: 4.133056133056133\nNon-Customer Mean Number of Patents: 3.4730127576054954\n\n\nThe histogram above compares the distribution of patent counts between firms that use Blueprinty software (iscustomer = 1) and those that do not (iscustomer = 0). A few key patterns emerge:\n\nThe distribution is right-skewed for both groups, with the bulk of firms holding between 0 and 6 patents.\nNon-customers are more concentrated around lower patent counts, particularly at 2-4 patents.\nCustomers of Blueprinty show a relatively flatter distribution, with more firms holding moderate to higher numbers of patents.\n\nThe summary statistics support this visual trend:\n\nNon-customers have 3.47 patents on average (n = 1,019)\nCustomers have 4.13 patents on average (n = 481)\n\nAlthough this is a modest difference (approximately 0.66 more patents on average), it is consistent with the idea that Blueprinty customers may be more successful or more prolific in obtaining patents. However, it is also possible that other factor, such as firm age or region, may explain the difference, so further analysis is needed before drawing causal conclusions.\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_style(\"whitegrid\")\n\nregion_props = pd.crosstab(blueprinty['region'], blueprinty['iscustomer'], normalize='index').reset_index()\nregion_props.columns = ['Region', 'Non-Customers', 'Customers']\nregion_props = pd.melt(region_props, id_vars='Region', var_name='Customer Type', value_name='Proportion')\n\nplt.figure(figsize=(8, 5))\nsns.barplot(data=region_props, x='Region', y='Proportion', hue='Customer Type')\nplt.title('Proportion of Firms by Region and Customer Status')\nplt.ylabel('Proportion')\nplt.xlabel('Region')\nplt.legend(title='Customer Type')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(6, 5))\nsns.boxplot(data=blueprinty, x='iscustomer', y='age')\nplt.title('Age of Firms by Customer Status')\nplt.xlabel('Customer Status (0 = Non-Customer, 1 = Customer)')\nplt.ylabel('Years Since Incorporation')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe region plot above shows notable variation in customer distribution across regions:\n\nIn most regions (Midwest, Northwest, South, Southwest), the majority of firms are non-customers, with Blueprinty customers making up only around 15–20% of firms.\nThe Northeast stands out as an exception, where nearly half of the firms are Blueprinty customers. This suggests that Blueprinty may have a stronger market presence or better adoption in that region.\n\nThe boxplot comparing firm age by customer status shows:\n\nThe median age of customers and non-customers is very similar, both around 25–27 years.\nThe spread of firm ages is also comparable, though customers appear to have a slightly higher upper range, with some firms approaching 50 years of age.\nThere’s no strong visual evidence of systematic age differences between customers and non-customers, but customers might skew slightly older on average.\n\nTogether, these plots suggest that while age may not be a major confounding factor, region could be, especially due to the Northeast’s unusual customer concentration. This is important to account for in any causal claims about Blueprinty’s effectiveness.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\nFor a Poisson model where \\(Y_i \\sim \\text{Poisson}(\\lambda)\\), the probability mass function is:\n\\[\nf(Y_i | \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nAssuming independence across \\(n\\) observations, the likelihood is:\n\\[\nL(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nTaking the natural log of the likelihood (log-likelihood):\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right)\n\\]\n\nfrom scipy.special import gammaln\n\ndef poisson_log_likelihood(lmbda, y):\n    if lmbda &lt;= 0:\n        return -np.inf\n    return np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1)) \n\n\n\n\n\n\nimport numpy as np\n\ny = blueprinty['patents'].values\n\nlambda_vals = np.linspace(0.1, 10, 200)\n\nlog_liks = [poisson_log_likelihood(lmbda, y) for lmbda in lambda_vals]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, log_liks)\nplt.title(\"Log-Likelihood of Poisson Model vs Lambda\")\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nTo find the MLE of \\(\\lambda\\), we take the log-likelihood:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right)\n\\]\nNow take the derivative with respect to \\(\\lambda\\):\n\\[\n\\frac{d\\ell}{d\\lambda} = \\sum_{i=1}^n \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n= -n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i\n\\]\nSet the derivative equal to zero and solve:\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = 0\n\\]\n\\[\n\\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = n\n\\]\n\\[\n\\lambda_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n\\]\nThus, the maximum likelihood estimate of \\(\\lambda\\) is simply the sample mean of \\(Y\\), which makes intuitive sense: the Poisson distribution models counts with mean \\(\\lambda\\), so the best estimate of \\(\\lambda\\) is just the average count in the data.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\n\n\n\nfrom scipy.optimize import minimize_scalar\nfrom scipy.special import gammaln\n\ndef neg_poisson_log_likelihood(lmbda, y):\n    if lmbda &lt;= 0:\n        return np.inf\n    return -np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))\n\ny = blueprinty['patents'].values\n\nresult = minimize_scalar(neg_poisson_log_likelihood, bounds=(0.1, 10), args=(y,), method='bounded')\n\nlambda_mle = result.x\nlambda_mle\n\n3.684666226134359\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\n\ndef poisson_regression_log_likelihood(beta, X, y):\n    beta = np.asarray(beta, dtype=np.float64)\n    X = np.asarray(X, dtype=np.float64)\n    y = np.asarray(y, dtype=np.float64)\n\n    eta = X @ beta\n\n    eta = np.clip(eta, -20, 20)\n    lam = np.exp(eta)\n\n    log_lik = np.sum(-lam + y * eta - gammaln(y + 1))\n    return -log_lik\n\n\n\n\n\nblueprinty['age_squared'] = blueprinty['age']**2\n\nregion_dummies = pd.get_dummies(blueprinty['region'], drop_first=True)\n\nX = pd.concat([\n    pd.Series(1, index=blueprinty.index, name='intercept'),\n    blueprinty[['age', 'age_squared', 'iscustomer']],\n    region_dummies\n], axis=1)\n\ny = blueprinty['patents'].values\nX_matrix = X.values\n\n\n\n\n\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\n\nX_matrix = np.asarray(X.values, dtype=np.float64)\ny = np.asarray(blueprinty['patents'].values, dtype=np.float64)\n\n\nnp.random.seed(42)\nbeta_init = np.random.normal(0, 0.1, size=X_matrix.shape[1])\n\nresult = minimize(\n    poisson_regression_log_likelihood,\n    beta_init,\n    args=(X_matrix, y),\n    method='BFGS'\n)\n\nbeta_hat = result.x\nbeta_hat\n\narray([ 0.04630095, -0.07210522, -0.94354045,  0.15102042, -0.02470711,\n       -0.02376197,  0.15708977,  0.07607481])\n\n\n\n\n\n\nvcov = result.hess_inv\nstandard_errors = np.sqrt(np.diag(vcov))\n\nsummary_table = pd.DataFrame({\n    'Coefficient': beta_hat,\n    'Std. Error': standard_errors\n}, index=X.columns)\n\nsummary_table\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nintercept\n0.046301\n1.0\n\n\nage\n-0.072105\n1.0\n\n\nage_squared\n-0.943540\n1.0\n\n\niscustomer\n0.151020\n1.0\n\n\nNortheast\n-0.024707\n1.0\n\n\nNorthwest\n-0.023762\n1.0\n\n\nSouth\n0.157090\n1.0\n\n\nSouthwest\n0.076075\n1.0\n\n\n\n\n\n\n\nNOT SURE IF SUMMARY TABLE IS RIGHT\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\nWRONG TOO!!!\n\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "blog/project2/index.html#blueprinty-case-study-hw-2",
    "href": "blog/project2/index.html#blueprinty-case-study-hw-2",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nblueprinty = pd.read_csv(\"/home/jovyan/Desktop/Marketing Analytics Website : HW's/blueprinty.csv\")\nairbnb = pd.read_csv(\"/home/jovyan/Desktop/Marketing Analytics Website : HW's/airbnb.csv\")\n\nblueprinty\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n...\n...\n...\n...\n...\n\n\n1495\n2\nNortheast\n18.5\n1\n\n\n1496\n3\nSouthwest\n22.5\n0\n\n\n1497\n4\nSouthwest\n17.0\n0\n\n\n1498\n3\nSouth\n29.0\n0\n\n\n1499\n1\nSouth\n39.0\n0\n\n\n\n\n1500 rows × 4 columns\n\n\n\n\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\ncustomers = blueprinty[blueprinty['iscustomer'] == 1]\nnon_customers = blueprinty[blueprinty['iscustomer'] == 0]\n\nplt.figure(figsize=(10, 5))\nplt.hist(customers['patents'], bins=20, alpha=0.6, label='Customers', edgecolor='black')\nplt.hist(non_customers['patents'], bins=20, alpha=0.6, label='Non-Customers', edgecolor='black')\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histogram of Number of Patents by Customer Status')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nmean_customers = customers['patents'].mean()\nmean_non_customers = non_customers['patents'].mean()\n\nprint(f'Customer Mean Number of Patents: {mean_customers}')\nprint(f'Non-Customer Mean Number of Patents: {mean_non_customers}')\n\n\n\n\n\n\n\n\nCustomer Mean Number of Patents: 4.133056133056133\nNon-Customer Mean Number of Patents: 3.4730127576054954\n\n\nThe histogram above compares the distribution of patent counts between firms that use Blueprinty software (iscustomer = 1) and those that do not (iscustomer = 0). A few key patterns emerge:\n\nThe distribution is right-skewed for both groups, with the bulk of firms holding between 0 and 6 patents.\nNon-customers are more concentrated around lower patent counts, particularly at 2-4 patents.\nCustomers of Blueprinty show a relatively flatter distribution, with more firms holding moderate to higher numbers of patents.\n\nThe summary statistics support this visual trend:\n\nNon-customers have 3.47 patents on average (n = 1,019)\nCustomers have 4.13 patents on average (n = 481)\n\nAlthough this is a modest difference (approximately 0.66 more patents on average), it is consistent with the idea that Blueprinty customers may be more successful or more prolific in obtaining patents. However, it is also possible that other factor, such as firm age or region, may explain the difference, so further analysis is needed before drawing causal conclusions.\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_style(\"whitegrid\")\n\nregion_props = pd.crosstab(blueprinty['region'], blueprinty['iscustomer'], normalize='index').reset_index()\nregion_props.columns = ['Region', 'Non-Customers', 'Customers']\nregion_props = pd.melt(region_props, id_vars='Region', var_name='Customer Type', value_name='Proportion')\n\nplt.figure(figsize=(8, 5))\nsns.barplot(data=region_props, x='Region', y='Proportion', hue='Customer Type')\nplt.title('Proportion of Firms by Region and Customer Status')\nplt.ylabel('Proportion')\nplt.xlabel('Region')\nplt.legend(title='Customer Type')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(6, 5))\nsns.boxplot(data=blueprinty, x='iscustomer', y='age')\nplt.title('Age of Firms by Customer Status')\nplt.xlabel('Customer Status (0 = Non-Customer, 1 = Customer)')\nplt.ylabel('Years Since Incorporation')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe region plot above shows notable variation in customer distribution across regions:\n\nIn most regions (Midwest, Northwest, South, Southwest), the majority of firms are non-customers, with Blueprinty customers making up only around 15–20% of firms.\nThe Northeast stands out as an exception, where nearly half of the firms are Blueprinty customers. This suggests that Blueprinty may have a stronger market presence or better adoption in that region.\n\nThe boxplot comparing firm age by customer status shows:\n\nThe median age of customers and non-customers is very similar, both around 25–27 years.\nThe spread of firm ages is also comparable, though customers appear to have a slightly higher upper range, with some firms approaching 50 years of age.\nThere’s no strong visual evidence of systematic age differences between customers and non-customers, but customers might skew slightly older on average.\n\nTogether, these plots suggest that while age may not be a major confounding factor, region could be, especially due to the Northeast’s unusual customer concentration. This is important to account for in any causal claims about Blueprinty’s effectiveness.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\nFor a Poisson model where \\(Y_i \\sim \\text{Poisson}(\\lambda)\\), the probability mass function is:\n\\[\nf(Y_i | \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nAssuming independence across \\(n\\) observations, the likelihood is:\n\\[\nL(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nTaking the natural log of the likelihood (log-likelihood):\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right)\n\\]\n\nfrom scipy.special import gammaln\n\ndef poisson_log_likelihood(lmbda, y):\n    if lmbda &lt;= 0:\n        return -np.inf\n    return np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1)) \n\n\n\n\n\n\nimport numpy as np\n\ny = blueprinty['patents'].values\n\nlambda_vals = np.linspace(0.1, 10, 200)\n\nlog_liks = [poisson_log_likelihood(lmbda, y) for lmbda in lambda_vals]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, log_liks)\nplt.title(\"Log-Likelihood of Poisson Model vs Lambda\")\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nTo find the MLE of \\(\\lambda\\), we take the log-likelihood:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right)\n\\]\nNow take the derivative with respect to \\(\\lambda\\):\n\\[\n\\frac{d\\ell}{d\\lambda} = \\sum_{i=1}^n \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n= -n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i\n\\]\nSet the derivative equal to zero and solve:\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = 0\n\\]\n\\[\n\\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = n\n\\]\n\\[\n\\lambda_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n\\]\nThus, the maximum likelihood estimate of \\(\\lambda\\) is simply the sample mean of \\(Y\\), which makes intuitive sense: the Poisson distribution models counts with mean \\(\\lambda\\), so the best estimate of \\(\\lambda\\) is just the average count in the data.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\n\n\n\nfrom scipy.optimize import minimize_scalar\nfrom scipy.special import gammaln\n\ndef neg_poisson_log_likelihood(lmbda, y):\n    if lmbda &lt;= 0:\n        return np.inf\n    return -np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))\n\ny = blueprinty['patents'].values\n\nresult = minimize_scalar(neg_poisson_log_likelihood, bounds=(0.1, 10), args=(y,), method='bounded')\n\nlambda_mle = result.x\nlambda_mle\n\n3.684666226134359\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\n\ndef poisson_regression_log_likelihood(beta, X, y):\n    beta = np.asarray(beta, dtype=np.float64)\n    X = np.asarray(X, dtype=np.float64)\n    y = np.asarray(y, dtype=np.float64)\n\n    eta = X @ beta\n\n    eta = np.clip(eta, -20, 20)\n    lam = np.exp(eta)\n\n    log_lik = np.sum(-lam + y * eta - gammaln(y + 1))\n    return -log_lik\n\n\n\n\n\nblueprinty['age_squared'] = blueprinty['age']**2\n\nregion_dummies = pd.get_dummies(blueprinty['region'], drop_first=True)\n\nX = pd.concat([\n    pd.Series(1, index=blueprinty.index, name='intercept'),\n    blueprinty[['age', 'age_squared', 'iscustomer']],\n    region_dummies\n], axis=1)\n\ny = blueprinty['patents'].values\nX_matrix = X.values\n\n\n\n\n\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\n\nX_matrix = np.asarray(X.values, dtype=np.float64)\ny = np.asarray(blueprinty['patents'].values, dtype=np.float64)\n\n\nnp.random.seed(42)\nbeta_init = np.random.normal(0, 0.1, size=X_matrix.shape[1])\n\nresult = minimize(\n    poisson_regression_log_likelihood,\n    beta_init,\n    args=(X_matrix, y),\n    method='BFGS'\n)\n\nbeta_hat = result.x\nbeta_hat\n\narray([ 0.04630095, -0.07210522, -0.94354045,  0.15102042, -0.02470711,\n       -0.02376197,  0.15708977,  0.07607481])\n\n\n\n\n\n\nvcov = result.hess_inv\nstandard_errors = np.sqrt(np.diag(vcov))\n\nsummary_table = pd.DataFrame({\n    'Coefficient': beta_hat,\n    'Std. Error': standard_errors\n}, index=X.columns)\n\nsummary_table\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nintercept\n0.046301\n1.0\n\n\nage\n-0.072105\n1.0\n\n\nage_squared\n-0.943540\n1.0\n\n\niscustomer\n0.151020\n1.0\n\n\nNortheast\n-0.024707\n1.0\n\n\nNorthwest\n-0.023762\n1.0\n\n\nSouth\n0.157090\n1.0\n\n\nSouthwest\n0.076075\n1.0\n\n\n\n\n\n\n\nNOT SURE IF SUMMARY TABLE IS RIGHT\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\nWRONG TOO!!!\n\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "blog/project2/index.html#airbnb-case-study",
    "href": "blog/project2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided."
  }
]